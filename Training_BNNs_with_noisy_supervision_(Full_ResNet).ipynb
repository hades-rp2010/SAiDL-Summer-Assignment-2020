{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training BNNs with noisy supervision (Full ResNet)",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "55d7ac7cb60540b0872653c33b20004e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e3f00c124c3346fe8ec8165463aa7934",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_af7178d5e89a4881a533ba3c785fc209",
              "IPY_MODEL_a5d14ce3ecbe4a769045160d2cef51c2"
            ]
          }
        },
        "e3f00c124c3346fe8ec8165463aa7934": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "af7178d5e89a4881a533ba3c785fc209": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6a85de9c66c3472dafadcc52a8cf49e8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3062682eb277435b908f6e7d6f613a02"
          }
        },
        "a5d14ce3ecbe4a769045160d2cef51c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_88fd58e5bfac4abb93439d929777caaa",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:30&lt;00:00, 17377270.09it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_be3f4e71c08b48be990ad436e6641a2a"
          }
        },
        "6a85de9c66c3472dafadcc52a8cf49e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3062682eb277435b908f6e7d6f613a02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "88fd58e5bfac4abb93439d929777caaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "be3f4e71c08b48be990ad436e6641a2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn-rPFpPa1ms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZ0a-d5lqznX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8e477ec8-8c07-42f9-8487-1fdb2bdc3ec1"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvnG7FQdLXTY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "72dac6a8-1acd-4591-a6ef-ac8da5285c84"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD6KeIw94XQV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def binarize(x):\n",
        "    return x.sign()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiQh5AHm3ejQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BinConv2d(nn.Conv2d):\n",
        "    def __init__(self, bin_fn, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.bin_fn = bin_fn\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = binarize(x)\n",
        "        if self.bin_fn == binarize:\n",
        "            self.weight = nn.Parameter(self.bin_fn(self.weight))\n",
        "        else:\n",
        "            self.bin_fn = Mapping(x.shape[1]).to(device)\n",
        "            self.weight = nn.Parameter(self.bin_fn(self.weight))\n",
        "        out = nn.functional.conv2d(x, self.weight, None, self.stride, self.padding, self.dilation, self.groups)\n",
        "        return out"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyWkbUkECiMT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LambdaLayer(nn.Module):\n",
        "    def __init__(self, lambd):\n",
        "        super(LambdaLayer, self).__init__()\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lambd(x)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = LambdaLayer(lambda x: F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = nn.ReLU()(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = nn.ReLU()(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.inflate = 4\n",
        "        self.in_planes = 16 * self.inflate\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16 * self.inflate, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16 * self.inflate)\n",
        "        self.res1 = self._make_layer(block, 16 * self.inflate, num_blocks[0], stride=1)\n",
        "        self.res2 = self._make_layer(block, 32 * self.inflate, num_blocks[1], stride=2)\n",
        "        self.res3 = self._make_layer(block, 64 * self.inflate, num_blocks[2], stride=2)\n",
        "        self.res_num = len(num_blocks)\n",
        "        if self.res_num==4:\n",
        "            self.res4 = self._make_layer(block, 128*self.inflate, num_blocks[3], stride=2)\n",
        "            self.linear = nn.Linear(128 * self.inflate, num_classes)\n",
        "        else:\n",
        "            self.linear = nn.Linear(64 * self.inflate, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = nn.ReLU()(self.bn1(self.conv1(x)))\n",
        "        out = self.res1(out)\n",
        "        out = self.res2(out)\n",
        "        out = self.res3(out)\n",
        "        if self.res_num==4:\n",
        "            out = self.res4(out)\n",
        "        out = F.avg_pool2d(out, out.size()[3])\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return nn.Softmax(dim = -1)(out)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UV8PwQRAQluv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BinBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, bin_fn = \"map\"):\n",
        "        super(BinBlock, self).__init__()\n",
        "        self.conv1 = BinConv2d(bin_fn, in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = BinConv2d(bin_fn, planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.bin_fn = bin_fn\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = LambdaLayer(lambda x: F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = nn.Hardtanh()(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += self.shortcut(x)\n",
        "        out = nn.Hardtanh()(out)\n",
        "        return out\n",
        "\n",
        "class Mapping(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, 2 * channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(2 * channels)\n",
        "        self.conv2 = nn.Conv2d(2 * channels, 2 * channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(2 * channels)\n",
        "        self.conv3 = nn.Conv2d(2 * channels, channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.relu(self.bn1(out))\n",
        "        out = self.conv2(out)\n",
        "        out = self.relu(self.bn2(out))\n",
        "        out = self.conv3(out)\n",
        "        return out\n",
        "\n",
        "class BinResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes, bin_fn):\n",
        "        super(BinResNet, self).__init__()\n",
        "        self.bin_fn = bin_fn\n",
        "        self.inflate = 4\n",
        "        self.in_planes = 16 * self.inflate\n",
        "        self.conv1 = nn.Conv2d(3, 16 * self.inflate, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16 * self.inflate)\n",
        "        self.res1 = self._make_layer(block, 16 * self.inflate, num_blocks[0], stride=1)\n",
        "        self.res2 = self._make_layer(block, 32 * self.inflate, num_blocks[1], stride=2)\n",
        "        self.res3 = self._make_layer(block, 64 * self.inflate, num_blocks[2], stride=2)\n",
        "        self.res_num = len(num_blocks)\n",
        "        if self.res_num==4:\n",
        "            self.res4 = self._make_layer(block, 128*self.inflate, num_blocks[3], stride=2)\n",
        "            self.linear = nn.Linear(128 * self.inflate, num_classes)\n",
        "        else:\n",
        "            self.linear = nn.Linear(64 * self.inflate, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride, self.bin_fn))\n",
        "            self.in_planes = planes * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.bn1(self.conv1(x))\n",
        "        out = binarize(out)\n",
        "        out = self.res1(out)\n",
        "        out = self.res2(out)\n",
        "        out = self.res3(out)\n",
        "        if self.res_num==4:\n",
        "            out = self.res4(out)\n",
        "        out = F.avg_pool2d(out, out.size()[3])\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return nn.Softmax(dim = -1)(out)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HWwTO3GOXCN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "19e00b19-8d67-496d-fae0-0cf4ed7090a1"
      },
      "source": [
        "def test():\n",
        "    net = ResNet(BasicBlock, [3, 3, 3], 1000)\n",
        "    x = torch.rand(2, 3, 224, 224)\n",
        "    y = net(x)\n",
        "    print(net.state_dict().keys())\n",
        "    print(y.shape)\n",
        "\n",
        "def test_bin():\n",
        "    binnet = BinResNet(BinBlock, [3, 3, 3], 100, bin_fn = \"map\").to(device)\n",
        "    x = torch.rand(2, 3, 224, 224).to(device)\n",
        "    # print(binnet)\n",
        "    y = binnet(x)\n",
        "    print(y.shape)\n",
        "\n",
        "test()\n",
        "test_bin()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "odict_keys(['conv1.weight', 'bn1.weight', 'bn1.bias', 'bn1.running_mean', 'bn1.running_var', 'bn1.num_batches_tracked', 'res1.0.conv1.weight', 'res1.0.bn1.weight', 'res1.0.bn1.bias', 'res1.0.bn1.running_mean', 'res1.0.bn1.running_var', 'res1.0.bn1.num_batches_tracked', 'res1.0.conv2.weight', 'res1.0.bn2.weight', 'res1.0.bn2.bias', 'res1.0.bn2.running_mean', 'res1.0.bn2.running_var', 'res1.0.bn2.num_batches_tracked', 'res1.1.conv1.weight', 'res1.1.bn1.weight', 'res1.1.bn1.bias', 'res1.1.bn1.running_mean', 'res1.1.bn1.running_var', 'res1.1.bn1.num_batches_tracked', 'res1.1.conv2.weight', 'res1.1.bn2.weight', 'res1.1.bn2.bias', 'res1.1.bn2.running_mean', 'res1.1.bn2.running_var', 'res1.1.bn2.num_batches_tracked', 'res1.2.conv1.weight', 'res1.2.bn1.weight', 'res1.2.bn1.bias', 'res1.2.bn1.running_mean', 'res1.2.bn1.running_var', 'res1.2.bn1.num_batches_tracked', 'res1.2.conv2.weight', 'res1.2.bn2.weight', 'res1.2.bn2.bias', 'res1.2.bn2.running_mean', 'res1.2.bn2.running_var', 'res1.2.bn2.num_batches_tracked', 'res2.0.conv1.weight', 'res2.0.bn1.weight', 'res2.0.bn1.bias', 'res2.0.bn1.running_mean', 'res2.0.bn1.running_var', 'res2.0.bn1.num_batches_tracked', 'res2.0.conv2.weight', 'res2.0.bn2.weight', 'res2.0.bn2.bias', 'res2.0.bn2.running_mean', 'res2.0.bn2.running_var', 'res2.0.bn2.num_batches_tracked', 'res2.1.conv1.weight', 'res2.1.bn1.weight', 'res2.1.bn1.bias', 'res2.1.bn1.running_mean', 'res2.1.bn1.running_var', 'res2.1.bn1.num_batches_tracked', 'res2.1.conv2.weight', 'res2.1.bn2.weight', 'res2.1.bn2.bias', 'res2.1.bn2.running_mean', 'res2.1.bn2.running_var', 'res2.1.bn2.num_batches_tracked', 'res2.2.conv1.weight', 'res2.2.bn1.weight', 'res2.2.bn1.bias', 'res2.2.bn1.running_mean', 'res2.2.bn1.running_var', 'res2.2.bn1.num_batches_tracked', 'res2.2.conv2.weight', 'res2.2.bn2.weight', 'res2.2.bn2.bias', 'res2.2.bn2.running_mean', 'res2.2.bn2.running_var', 'res2.2.bn2.num_batches_tracked', 'res3.0.conv1.weight', 'res3.0.bn1.weight', 'res3.0.bn1.bias', 'res3.0.bn1.running_mean', 'res3.0.bn1.running_var', 'res3.0.bn1.num_batches_tracked', 'res3.0.conv2.weight', 'res3.0.bn2.weight', 'res3.0.bn2.bias', 'res3.0.bn2.running_mean', 'res3.0.bn2.running_var', 'res3.0.bn2.num_batches_tracked', 'res3.1.conv1.weight', 'res3.1.bn1.weight', 'res3.1.bn1.bias', 'res3.1.bn1.running_mean', 'res3.1.bn1.running_var', 'res3.1.bn1.num_batches_tracked', 'res3.1.conv2.weight', 'res3.1.bn2.weight', 'res3.1.bn2.bias', 'res3.1.bn2.running_mean', 'res3.1.bn2.running_var', 'res3.1.bn2.num_batches_tracked', 'res3.2.conv1.weight', 'res3.2.bn1.weight', 'res3.2.bn1.bias', 'res3.2.bn1.running_mean', 'res3.2.bn1.running_var', 'res3.2.bn1.num_batches_tracked', 'res3.2.conv2.weight', 'res3.2.bn2.weight', 'res3.2.bn2.bias', 'res3.2.bn2.running_mean', 'res3.2.bn2.running_var', 'res3.2.bn2.num_batches_tracked', 'linear.weight', 'linear.bias'])\n",
            "torch.Size([2, 1000])\n",
            "torch.Size([2, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYZV2YWurlQX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "55d7ac7cb60540b0872653c33b20004e",
            "e3f00c124c3346fe8ec8165463aa7934",
            "af7178d5e89a4881a533ba3c785fc209",
            "a5d14ce3ecbe4a769045160d2cef51c2",
            "6a85de9c66c3472dafadcc52a8cf49e8",
            "3062682eb277435b908f6e7d6f613a02",
            "88fd58e5bfac4abb93439d929777caaa",
            "be3f4e71c08b48be990ad436e6641a2a"
          ]
        },
        "outputId": "191b35d3-2679-44eb-8012-53c9e119f1c3"
      },
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Image preprocessing modules\n",
        "transform = transforms.Compose([\n",
        "    transforms.Pad(4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32),\n",
        "    transforms.ToTensor()])\n",
        "\n",
        "# CIFAR-10 dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
        "                                             train=True, \n",
        "                                             transform=transform,\n",
        "                                             download=True)\n",
        "\n",
        "train_set, val_set = torch.utils.data.random_split(train_dataset, [int(0.8 * len(train_dataset)), int(0.2 * len(train_dataset))])\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
        "                                            train=False, \n",
        "                                            transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
        "                                           batch_size=128, \n",
        "                                           shuffle=True)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_set,\n",
        "                                         batch_size=128,\n",
        "                                         shuffle = True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=128, \n",
        "                                          shuffle=False)        "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../../data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55d7ac7cb60540b0872653c33b20004e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../../data/cifar-10-python.tar.gz to ../../data/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHMNGM62qhWZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bin = [\"res\", \"conv\", \"weight\"]\n",
        "notbin = [\"conv1.weight\", \"linear.weight\", \"linear.bias\"]\n",
        "map = [\"bin_fn\"]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nVdGNMRRWJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_bin_dict(binnet):\n",
        "    bin_fns = {}\n",
        "    bin_fns[\"res1.0.conv1\"] = binnet.res1[0].conv1.bin_fn\n",
        "    bin_fns[\"res1.0.conv2\"] = binnet.res1[0].conv1.bin_fn\n",
        "    bin_fns[\"res1.1.conv1\"] = binnet.res1[1].conv1.bin_fn\n",
        "    bin_fns[\"res1.1.conv2\"] = binnet.res1[1].conv1.bin_fn\n",
        "    bin_fns[\"res1.2.conv1\"] = binnet.res1[2].conv1.bin_fn\n",
        "    bin_fns[\"res1.2.conv2\"] = binnet.res1[2].conv1.bin_fn\n",
        "    bin_fns[\"res2.0.conv1\"] = binnet.res2[0].conv1.bin_fn\n",
        "    bin_fns[\"res2.0.conv2\"] = binnet.res2[0].conv1.bin_fn\n",
        "    bin_fns[\"res2.1.conv1\"] = binnet.res2[1].conv1.bin_fn\n",
        "    bin_fns[\"res2.1.conv2\"] = binnet.res2[1].conv1.bin_fn\n",
        "    bin_fns[\"res2.2.conv1\"] = binnet.res2[2].conv1.bin_fn\n",
        "    bin_fns[\"res2.2.conv2\"] = binnet.res2[2].conv1.bin_fn\n",
        "    bin_fns[\"res3.0.conv1\"] = binnet.res3[0].conv1.bin_fn\n",
        "    bin_fns[\"res3.0.conv2\"] = binnet.res3[0].conv1.bin_fn\n",
        "    bin_fns[\"res3.1.conv1\"] = binnet.res3[1].conv1.bin_fn\n",
        "    bin_fns[\"res3.1.conv2\"] = binnet.res3[1].conv1.bin_fn\n",
        "    bin_fns[\"res3.2.conv1\"] = binnet.res3[2].conv1.bin_fn\n",
        "    bin_fns[\"res3.2.conv2\"] = binnet.res3[2].conv1.bin_fn\n",
        "    return bin_fns"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpYbYpvIEQfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learn(net, binnet):\n",
        "    bin_fns = create_bin_dict(binnet)\n",
        "    noisy_loss = torch.tensor([0.0], device=device)\n",
        "    grad_aux_W = {}\n",
        "    for (name_q, q), (name_p, p) in zip(binnet.named_parameters(), net.named_parameters()):\n",
        "        if name_p==name_q:\n",
        "            if all([i in name_p for i in bin]):\n",
        "                W = p.data\n",
        "                W.requires_grad = True\n",
        "                Q = binarize(p.data)\n",
        "                bin_fn  = bin_fns[name_p.replace(\".weight\", \"\")]\n",
        "                Q_star = bin_fn(W)\n",
        "                Q_star.to(\"cpu\")\n",
        "                aux_loss = torch.sum((1 - rhof) * nn.MSELoss()(Q, Q_star) - rhof * nn.MSELoss()(-Q, Q_star) / (1 - 2 * rhof)) # li = aux_loss\n",
        "                aux_loss.backward() # should give d(li)/d(theta) theta - map params\n",
        "\n",
        "                grad_aux_W[name_p] = W.grad.data # should store d(li)/dW\n",
        "                noisy_loss += aux_loss\n",
        "\n",
        "                assert q.data.shape == Q_star.shape\n",
        "                q.data = Q_star\n",
        "    \n",
        "    return noisy_loss, binnet, grad_aux_W"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W338AA3l6H_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def base_train(net, binnet):\n",
        "    for (name_q, q), (name_p, p) in zip(binnet.named_parameters(), net.named_parameters()):\n",
        "        if name_q==name_p:\n",
        "            if all([i in name_p for i in bin]):\n",
        "                q.data = p.data\n",
        "            elif name_p in notbin:\n",
        "                q.data = p.data\n",
        "\n",
        "    return binnet"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DHyxcZXkBI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = ResNet(BasicBlock, [3, 3, 3], 10).to(device)\n",
        "binnet = BinResNet(BinBlock, [3, 3, 3], 10, bin_fn=binarize).to(device)\n",
        "binnet_optim = torch.optim.SGD(binnet.parameters(), lr = 0.1, momentum = 0.9)\n",
        "net_optim = torch.optim.SGD(net.parameters(), lr = 0.1, momentum =0.9)\n",
        "rhof = 0.005"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mA9FfPQ2kFc-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrain_losses = []\n",
        "train_losses = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BkSJPHj2kTl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0b8ca83a-eb8d-4d1a-909c-9ebaf9a9d704"
      },
      "source": [
        "num_pretrain_epochs=400\n",
        "for epoch in range(num_pretrain_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # print(i)\n",
        "        images = images.to(device)\n",
        "        # print(images.shape)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        binnet = base_train(net, binnet)\n",
        "\n",
        "        out = binnet(images)\n",
        "        # print(out.shape)\n",
        "        ce_loss = nn.CrossEntropyLoss()(out, labels)\n",
        "        \n",
        "        binnet_optim.zero_grad()\n",
        "        net_optim.zero_grad()\n",
        "        ce_loss.backward()\n",
        "        nn.utils.clip_grad_value_(binnet.parameters(), 1)\n",
        "\n",
        "        for p, (name, q) in zip(net.parameters(), binnet.named_parameters()):\n",
        "            if q.grad is not None:\n",
        "                p.grad = q.grad\n",
        "\n",
        "        binnet_optim.step()\n",
        "        net_optim.step()\n",
        "        trainaccuracy = torch.sum(labels==torch.argmax(out, dim=-1)).to(\"cpu\").item()/len(images)\n",
        "\n",
        "    if (epoch+1)%1==0:\n",
        "        with torch.no_grad():\n",
        "            x = torch.tensor([0.0], device = device)\n",
        "            for images, labels in val_loader:\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                # print(labels.shape)\n",
        "                out = binnet(images)\n",
        "                # print(torch.argmax(out, dim = -1).shape)\n",
        "                x += torch.sum(labels == torch.argmax(out, dim = -1))\n",
        "            valaccuracy = x.to(\"cpu\").item()/10000\n",
        "        print(f\"Epoch [{epoch+1}/{num_pretrain_epochs}] Loss = {ce_loss.item()} TrainAccuracy = {trainaccuracy} ValAccuracy = {valaccuracy}\")\n",
        "    \n",
        "    if (epoch+1) % 10 == 0:\n",
        "        torch.save({\"epoch\": epoch+1, \"net_state_dict\": net.state_dict(), \"binnet_state_dict\": binnet.state_dict()}, \"/content/drive/My Drive/Artificial intelligence/bigmodel.th\")\n",
        "        print(f\"{epoch+1} saved\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/400] Loss = 2.2652363777160645 TrainAccuracy = 0.125 ValAccuracy = 0.1912\n",
            "Epoch [2/400] Loss = 2.2258291244506836 TrainAccuracy = 0.21875 ValAccuracy = 0.2069\n",
            "Epoch [3/400] Loss = 2.2424449920654297 TrainAccuracy = 0.171875 ValAccuracy = 0.2262\n",
            "Epoch [4/400] Loss = 2.218268632888794 TrainAccuracy = 0.296875 ValAccuracy = 0.228\n",
            "Epoch [5/400] Loss = 2.2504844665527344 TrainAccuracy = 0.1875 ValAccuracy = 0.2378\n",
            "Epoch [6/400] Loss = 2.1801698207855225 TrainAccuracy = 0.296875 ValAccuracy = 0.2416\n",
            "Epoch [7/400] Loss = 2.232516050338745 TrainAccuracy = 0.1875 ValAccuracy = 0.2447\n",
            "Epoch [8/400] Loss = 2.209232807159424 TrainAccuracy = 0.265625 ValAccuracy = 0.2475\n",
            "Epoch [9/400] Loss = 2.200521230697632 TrainAccuracy = 0.234375 ValAccuracy = 0.253\n",
            "Epoch [10/400] Loss = 2.1866190433502197 TrainAccuracy = 0.234375 ValAccuracy = 0.2549\n",
            "10 saved\n",
            "Epoch [11/400] Loss = 2.1807570457458496 TrainAccuracy = 0.265625 ValAccuracy = 0.2559\n",
            "Epoch [12/400] Loss = 2.1015567779541016 TrainAccuracy = 0.359375 ValAccuracy = 0.269\n",
            "Epoch [13/400] Loss = 2.1679794788360596 TrainAccuracy = 0.265625 ValAccuracy = 0.2726\n",
            "Epoch [14/400] Loss = 2.1306705474853516 TrainAccuracy = 0.359375 ValAccuracy = 0.2723\n",
            "Epoch [15/400] Loss = 2.166785717010498 TrainAccuracy = 0.28125 ValAccuracy = 0.2771\n",
            "Epoch [16/400] Loss = 2.210101366043091 TrainAccuracy = 0.203125 ValAccuracy = 0.2778\n",
            "Epoch [17/400] Loss = 2.201348066329956 TrainAccuracy = 0.25 ValAccuracy = 0.2817\n",
            "Epoch [18/400] Loss = 2.183016777038574 TrainAccuracy = 0.265625 ValAccuracy = 0.2703\n",
            "Epoch [19/400] Loss = 2.1982390880584717 TrainAccuracy = 0.234375 ValAccuracy = 0.284\n",
            "Epoch [20/400] Loss = 2.2054712772369385 TrainAccuracy = 0.21875 ValAccuracy = 0.2885\n",
            "20 saved\n",
            "Epoch [21/400] Loss = 2.192257881164551 TrainAccuracy = 0.28125 ValAccuracy = 0.2858\n",
            "Epoch [22/400] Loss = 2.198659658432007 TrainAccuracy = 0.234375 ValAccuracy = 0.2832\n",
            "Epoch [23/400] Loss = 2.1504907608032227 TrainAccuracy = 0.28125 ValAccuracy = 0.292\n",
            "Epoch [24/400] Loss = 2.2265148162841797 TrainAccuracy = 0.25 ValAccuracy = 0.2879\n",
            "Epoch [25/400] Loss = 2.1847727298736572 TrainAccuracy = 0.234375 ValAccuracy = 0.29\n",
            "Epoch [26/400] Loss = 2.1251697540283203 TrainAccuracy = 0.359375 ValAccuracy = 0.2887\n",
            "Epoch [27/400] Loss = 2.141162633895874 TrainAccuracy = 0.328125 ValAccuracy = 0.2906\n",
            "Epoch [28/400] Loss = 2.1864020824432373 TrainAccuracy = 0.21875 ValAccuracy = 0.2971\n",
            "Epoch [29/400] Loss = 2.2571916580200195 TrainAccuracy = 0.21875 ValAccuracy = 0.297\n",
            "Epoch [30/400] Loss = 2.1214599609375 TrainAccuracy = 0.328125 ValAccuracy = 0.2956\n",
            "30 saved\n",
            "Epoch [31/400] Loss = 2.1862616539001465 TrainAccuracy = 0.21875 ValAccuracy = 0.2962\n",
            "Epoch [32/400] Loss = 2.01985764503479 TrainAccuracy = 0.453125 ValAccuracy = 0.2995\n",
            "Epoch [33/400] Loss = 2.2088496685028076 TrainAccuracy = 0.25 ValAccuracy = 0.3016\n",
            "Epoch [34/400] Loss = 2.134990692138672 TrainAccuracy = 0.296875 ValAccuracy = 0.3057\n",
            "Epoch [35/400] Loss = 2.0336647033691406 TrainAccuracy = 0.421875 ValAccuracy = 0.2967\n",
            "Epoch [36/400] Loss = 2.1629695892333984 TrainAccuracy = 0.28125 ValAccuracy = 0.3084\n",
            "Epoch [37/400] Loss = 2.1215670108795166 TrainAccuracy = 0.3125 ValAccuracy = 0.3044\n",
            "Epoch [38/400] Loss = 2.1696059703826904 TrainAccuracy = 0.25 ValAccuracy = 0.3055\n",
            "Epoch [39/400] Loss = 2.1256704330444336 TrainAccuracy = 0.359375 ValAccuracy = 0.3051\n",
            "Epoch [40/400] Loss = 2.1761395931243896 TrainAccuracy = 0.265625 ValAccuracy = 0.3048\n",
            "40 saved\n",
            "Epoch [41/400] Loss = 2.1037144660949707 TrainAccuracy = 0.359375 ValAccuracy = 0.3125\n",
            "Epoch [42/400] Loss = 2.155386447906494 TrainAccuracy = 0.28125 ValAccuracy = 0.3079\n",
            "Epoch [43/400] Loss = 2.2065582275390625 TrainAccuracy = 0.25 ValAccuracy = 0.3105\n",
            "Epoch [44/400] Loss = 2.123556613922119 TrainAccuracy = 0.359375 ValAccuracy = 0.3115\n",
            "Epoch [45/400] Loss = 2.149019718170166 TrainAccuracy = 0.296875 ValAccuracy = 0.3073\n",
            "Epoch [46/400] Loss = 2.1614177227020264 TrainAccuracy = 0.296875 ValAccuracy = 0.3083\n",
            "Epoch [47/400] Loss = 2.06618595123291 TrainAccuracy = 0.390625 ValAccuracy = 0.3188\n",
            "Epoch [48/400] Loss = 2.2040421962738037 TrainAccuracy = 0.234375 ValAccuracy = 0.3153\n",
            "Epoch [49/400] Loss = 2.127084255218506 TrainAccuracy = 0.3125 ValAccuracy = 0.3133\n",
            "Epoch [50/400] Loss = 2.0816092491149902 TrainAccuracy = 0.359375 ValAccuracy = 0.3166\n",
            "50 saved\n",
            "Epoch [51/400] Loss = 2.1461596488952637 TrainAccuracy = 0.296875 ValAccuracy = 0.3138\n",
            "Epoch [52/400] Loss = 2.1923389434814453 TrainAccuracy = 0.25 ValAccuracy = 0.321\n",
            "Epoch [53/400] Loss = 2.0762925148010254 TrainAccuracy = 0.421875 ValAccuracy = 0.3175\n",
            "Epoch [54/400] Loss = 2.093168258666992 TrainAccuracy = 0.359375 ValAccuracy = 0.3227\n",
            "Epoch [55/400] Loss = 2.1340365409851074 TrainAccuracy = 0.296875 ValAccuracy = 0.3205\n",
            "Epoch [56/400] Loss = 2.0841329097747803 TrainAccuracy = 0.375 ValAccuracy = 0.3219\n",
            "Epoch [57/400] Loss = 2.0758726596832275 TrainAccuracy = 0.375 ValAccuracy = 0.3283\n",
            "Epoch [58/400] Loss = 2.082049608230591 TrainAccuracy = 0.359375 ValAccuracy = 0.3306\n",
            "Epoch [59/400] Loss = 2.2061617374420166 TrainAccuracy = 0.25 ValAccuracy = 0.3217\n",
            "Epoch [60/400] Loss = 2.0982415676116943 TrainAccuracy = 0.34375 ValAccuracy = 0.3322\n",
            "60 saved\n",
            "Epoch [61/400] Loss = 2.084120988845825 TrainAccuracy = 0.359375 ValAccuracy = 0.3288\n",
            "Epoch [62/400] Loss = 2.132981777191162 TrainAccuracy = 0.34375 ValAccuracy = 0.3329\n",
            "Epoch [63/400] Loss = 2.1032989025115967 TrainAccuracy = 0.375 ValAccuracy = 0.3301\n",
            "Epoch [64/400] Loss = 2.1630942821502686 TrainAccuracy = 0.28125 ValAccuracy = 0.3266\n",
            "Epoch [65/400] Loss = 2.1824135780334473 TrainAccuracy = 0.28125 ValAccuracy = 0.3325\n",
            "Epoch [66/400] Loss = 2.053767681121826 TrainAccuracy = 0.40625 ValAccuracy = 0.3323\n",
            "Epoch [67/400] Loss = 2.161858081817627 TrainAccuracy = 0.28125 ValAccuracy = 0.3284\n",
            "Epoch [68/400] Loss = 2.1503982543945312 TrainAccuracy = 0.296875 ValAccuracy = 0.3343\n",
            "Epoch [69/400] Loss = 2.104339122772217 TrainAccuracy = 0.390625 ValAccuracy = 0.3342\n",
            "Epoch [70/400] Loss = 2.1644175052642822 TrainAccuracy = 0.265625 ValAccuracy = 0.3396\n",
            "70 saved\n",
            "Epoch [71/400] Loss = 2.1248764991760254 TrainAccuracy = 0.3125 ValAccuracy = 0.3336\n",
            "Epoch [72/400] Loss = 2.087489604949951 TrainAccuracy = 0.359375 ValAccuracy = 0.3353\n",
            "Epoch [73/400] Loss = 2.079279899597168 TrainAccuracy = 0.359375 ValAccuracy = 0.3368\n",
            "Epoch [74/400] Loss = 2.1714000701904297 TrainAccuracy = 0.265625 ValAccuracy = 0.3385\n",
            "Epoch [75/400] Loss = 2.0845704078674316 TrainAccuracy = 0.375 ValAccuracy = 0.3354\n",
            "Epoch [76/400] Loss = 2.026874303817749 TrainAccuracy = 0.421875 ValAccuracy = 0.3396\n",
            "Epoch [77/400] Loss = 2.094209671020508 TrainAccuracy = 0.328125 ValAccuracy = 0.3386\n",
            "Epoch [78/400] Loss = 2.1174612045288086 TrainAccuracy = 0.328125 ValAccuracy = 0.3381\n",
            "Epoch [79/400] Loss = 2.2243425846099854 TrainAccuracy = 0.21875 ValAccuracy = 0.338\n",
            "Epoch [80/400] Loss = 2.1762983798980713 TrainAccuracy = 0.265625 ValAccuracy = 0.3438\n",
            "80 saved\n",
            "Epoch [81/400] Loss = 2.1625030040740967 TrainAccuracy = 0.28125 ValAccuracy = 0.345\n",
            "Epoch [82/400] Loss = 2.2189879417419434 TrainAccuracy = 0.203125 ValAccuracy = 0.3472\n",
            "Epoch [83/400] Loss = 2.1552932262420654 TrainAccuracy = 0.265625 ValAccuracy = 0.3425\n",
            "Epoch [84/400] Loss = 2.0916945934295654 TrainAccuracy = 0.359375 ValAccuracy = 0.3424\n",
            "Epoch [85/400] Loss = 2.031129837036133 TrainAccuracy = 0.421875 ValAccuracy = 0.3417\n",
            "Epoch [86/400] Loss = 2.0378260612487793 TrainAccuracy = 0.4375 ValAccuracy = 0.3474\n",
            "Epoch [87/400] Loss = 2.1848483085632324 TrainAccuracy = 0.28125 ValAccuracy = 0.3417\n",
            "Epoch [88/400] Loss = 2.040264368057251 TrainAccuracy = 0.390625 ValAccuracy = 0.3501\n",
            "Epoch [89/400] Loss = 2.157351493835449 TrainAccuracy = 0.296875 ValAccuracy = 0.3493\n",
            "Epoch [90/400] Loss = 2.161996841430664 TrainAccuracy = 0.296875 ValAccuracy = 0.342\n",
            "90 saved\n",
            "Epoch [91/400] Loss = 2.0582613945007324 TrainAccuracy = 0.421875 ValAccuracy = 0.3518\n",
            "Epoch [92/400] Loss = 2.1129109859466553 TrainAccuracy = 0.34375 ValAccuracy = 0.3505\n",
            "Epoch [93/400] Loss = 2.2220520973205566 TrainAccuracy = 0.1875 ValAccuracy = 0.3517\n",
            "Epoch [94/400] Loss = 2.118887186050415 TrainAccuracy = 0.359375 ValAccuracy = 0.3465\n",
            "Epoch [95/400] Loss = 2.057482957839966 TrainAccuracy = 0.40625 ValAccuracy = 0.3593\n",
            "Epoch [96/400] Loss = 2.0371618270874023 TrainAccuracy = 0.421875 ValAccuracy = 0.3489\n",
            "Epoch [97/400] Loss = 2.118863105773926 TrainAccuracy = 0.3125 ValAccuracy = 0.3569\n",
            "Epoch [98/400] Loss = 2.1595964431762695 TrainAccuracy = 0.3125 ValAccuracy = 0.349\n",
            "Epoch [99/400] Loss = 2.0038676261901855 TrainAccuracy = 0.46875 ValAccuracy = 0.3564\n",
            "Epoch [100/400] Loss = 2.1032555103302 TrainAccuracy = 0.34375 ValAccuracy = 0.3521\n",
            "100 saved\n",
            "Epoch [101/400] Loss = 2.0727570056915283 TrainAccuracy = 0.390625 ValAccuracy = 0.3569\n",
            "Epoch [102/400] Loss = 1.9980159997940063 TrainAccuracy = 0.46875 ValAccuracy = 0.3504\n",
            "Epoch [103/400] Loss = 2.0821950435638428 TrainAccuracy = 0.375 ValAccuracy = 0.3608\n",
            "Epoch [104/400] Loss = 1.9420222043991089 TrainAccuracy = 0.546875 ValAccuracy = 0.3587\n",
            "Epoch [105/400] Loss = 2.03782320022583 TrainAccuracy = 0.421875 ValAccuracy = 0.3546\n",
            "Epoch [106/400] Loss = 2.084315776824951 TrainAccuracy = 0.40625 ValAccuracy = 0.3594\n",
            "Epoch [107/400] Loss = 2.107306480407715 TrainAccuracy = 0.359375 ValAccuracy = 0.3613\n",
            "Epoch [108/400] Loss = 2.0449342727661133 TrainAccuracy = 0.40625 ValAccuracy = 0.3667\n",
            "Epoch [109/400] Loss = 2.1214053630828857 TrainAccuracy = 0.3125 ValAccuracy = 0.3615\n",
            "Epoch [110/400] Loss = 2.0663368701934814 TrainAccuracy = 0.40625 ValAccuracy = 0.3601\n",
            "110 saved\n",
            "Epoch [111/400] Loss = 2.085132598876953 TrainAccuracy = 0.359375 ValAccuracy = 0.356\n",
            "Epoch [112/400] Loss = 2.2108709812164307 TrainAccuracy = 0.21875 ValAccuracy = 0.3663\n",
            "Epoch [113/400] Loss = 2.153613328933716 TrainAccuracy = 0.328125 ValAccuracy = 0.3686\n",
            "Epoch [114/400] Loss = 2.1079483032226562 TrainAccuracy = 0.328125 ValAccuracy = 0.3675\n",
            "Epoch [115/400] Loss = 2.1357662677764893 TrainAccuracy = 0.296875 ValAccuracy = 0.3623\n",
            "Epoch [116/400] Loss = 2.1210265159606934 TrainAccuracy = 0.328125 ValAccuracy = 0.3595\n",
            "Epoch [117/400] Loss = 2.1951770782470703 TrainAccuracy = 0.25 ValAccuracy = 0.3661\n",
            "Epoch [118/400] Loss = 1.973807454109192 TrainAccuracy = 0.46875 ValAccuracy = 0.3635\n",
            "Epoch [119/400] Loss = 1.999015212059021 TrainAccuracy = 0.46875 ValAccuracy = 0.3705\n",
            "Epoch [120/400] Loss = 2.01961612701416 TrainAccuracy = 0.4375 ValAccuracy = 0.3668\n",
            "120 saved\n",
            "Epoch [121/400] Loss = 2.024855136871338 TrainAccuracy = 0.421875 ValAccuracy = 0.3738\n",
            "Epoch [122/400] Loss = 2.0165417194366455 TrainAccuracy = 0.421875 ValAccuracy = 0.3704\n",
            "Epoch [123/400] Loss = 2.002655029296875 TrainAccuracy = 0.484375 ValAccuracy = 0.3738\n",
            "Epoch [124/400] Loss = 2.0258615016937256 TrainAccuracy = 0.4375 ValAccuracy = 0.372\n",
            "Epoch [125/400] Loss = 2.1454577445983887 TrainAccuracy = 0.328125 ValAccuracy = 0.3727\n",
            "Epoch [126/400] Loss = 2.101520538330078 TrainAccuracy = 0.328125 ValAccuracy = 0.3732\n",
            "Epoch [127/400] Loss = 2.118239641189575 TrainAccuracy = 0.34375 ValAccuracy = 0.3707\n",
            "Epoch [128/400] Loss = 2.050736427307129 TrainAccuracy = 0.390625 ValAccuracy = 0.3763\n",
            "Epoch [129/400] Loss = 2.1355817317962646 TrainAccuracy = 0.3125 ValAccuracy = 0.3723\n",
            "Epoch [130/400] Loss = 2.081899881362915 TrainAccuracy = 0.375 ValAccuracy = 0.3726\n",
            "130 saved\n",
            "Epoch [131/400] Loss = 2.0191948413848877 TrainAccuracy = 0.453125 ValAccuracy = 0.378\n",
            "Epoch [132/400] Loss = 1.986084222793579 TrainAccuracy = 0.484375 ValAccuracy = 0.3754\n",
            "Epoch [133/400] Loss = 2.05973744392395 TrainAccuracy = 0.390625 ValAccuracy = 0.3707\n",
            "Epoch [134/400] Loss = 1.9627084732055664 TrainAccuracy = 0.46875 ValAccuracy = 0.3717\n",
            "Epoch [135/400] Loss = 2.1112558841705322 TrainAccuracy = 0.328125 ValAccuracy = 0.3718\n",
            "Epoch [136/400] Loss = 2.2088587284088135 TrainAccuracy = 0.21875 ValAccuracy = 0.3765\n",
            "Epoch [137/400] Loss = 2.0846142768859863 TrainAccuracy = 0.375 ValAccuracy = 0.3728\n",
            "Epoch [138/400] Loss = 2.111546039581299 TrainAccuracy = 0.328125 ValAccuracy = 0.3708\n",
            "Epoch [139/400] Loss = 2.129654884338379 TrainAccuracy = 0.3125 ValAccuracy = 0.3646\n",
            "Epoch [140/400] Loss = 2.031371593475342 TrainAccuracy = 0.453125 ValAccuracy = 0.3778\n",
            "140 saved\n",
            "Epoch [141/400] Loss = 2.059856653213501 TrainAccuracy = 0.40625 ValAccuracy = 0.3814\n",
            "Epoch [142/400] Loss = 1.9967405796051025 TrainAccuracy = 0.46875 ValAccuracy = 0.3824\n",
            "Epoch [143/400] Loss = 2.1379528045654297 TrainAccuracy = 0.296875 ValAccuracy = 0.3737\n",
            "Epoch [144/400] Loss = 2.07658314704895 TrainAccuracy = 0.375 ValAccuracy = 0.3738\n",
            "Epoch [145/400] Loss = 2.077399969100952 TrainAccuracy = 0.34375 ValAccuracy = 0.3795\n",
            "Epoch [146/400] Loss = 2.05937123298645 TrainAccuracy = 0.40625 ValAccuracy = 0.3748\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBCM4b_wS7ln",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c1b4f8a6-9359-46b5-9989-e83a1397f56b"
      },
      "source": [
        "# if not trained to 400 epochs\n",
        "x = torch.load(\"/content/drive/My Drive/Artificial intelligence/bigmodel.th\")\n",
        "net.load_state_dict(x[\"net_state_dict\"])\n",
        "binnet.load_state_dict(x[\"binnet_state_dict\"])\n",
        "for epoch in range(x[\"epoch\"]-1, 400):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # print(i)\n",
        "        images = images.to(device)\n",
        "        # print(images.shape)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        binnet = base_train(net, binnet)\n",
        "\n",
        "        out = binnet(images)\n",
        "        # print(out.shape)\n",
        "        ce_loss = nn.CrossEntropyLoss()(out, labels)\n",
        "        \n",
        "        binnet_optim.zero_grad()\n",
        "        net_optim.zero_grad()\n",
        "        ce_loss.backward()\n",
        "        nn.utils.clip_grad_value_(binnet.parameters(), 1)\n",
        "\n",
        "        for p, (name, q) in zip(net.parameters(), binnet.named_parameters()):\n",
        "            if q.grad is not None:\n",
        "                p.grad = q.grad\n",
        "\n",
        "        binnet_optim.step()\n",
        "        net_optim.step()\n",
        "        trainaccuracy = torch.sum(labels==torch.argmax(out, dim=-1)).to(\"cpu\").item()/len(images)\n",
        "\n",
        "    if (epoch+1)%1==0:\n",
        "        with torch.no_grad():\n",
        "            x = torch.tensor([0.0], device = device)\n",
        "            for images, labels in val_loader:\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                # print(labels.shape)\n",
        "                out = binnet(images)\n",
        "                # print(torch.argmax(out, dim = -1).shape)\n",
        "                x += torch.sum(labels == torch.argmax(out, dim = -1))\n",
        "            valaccuracy = x.to(\"cpu\").item()/10000\n",
        "        print(f\"Epoch [{epoch+1}/400] Loss = {ce_loss.item()} TrainAccuracy = {trainaccuracy} ValAccuracy = {valaccuracy}\")\n",
        "    \n",
        "    if (epoch+1) % 10 == 0:\n",
        "        torch.save({\"epoch\": epoch+1, \"net_state_dict\": net.state_dict(), \"binnet_state_dict\": binnet.state_dict()}, \"/content/drive/My Drive/Artificial intelligence/bigmodel.th\")\n",
        "        print(f\"{epoch+1} saved\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [140/400] Loss = 2.078464984893799 TrainAccuracy = 0.375 ValAccuracy = 0.3779\n",
            "140 saved\n",
            "Epoch [141/400] Loss = 2.1387104988098145 TrainAccuracy = 0.34375 ValAccuracy = 0.3782\n",
            "Epoch [142/400] Loss = 2.0908939838409424 TrainAccuracy = 0.3125 ValAccuracy = 0.3818\n",
            "Epoch [143/400] Loss = 2.1459243297576904 TrainAccuracy = 0.328125 ValAccuracy = 0.3778\n",
            "Epoch [144/400] Loss = 2.148730754852295 TrainAccuracy = 0.3125 ValAccuracy = 0.3778\n",
            "Epoch [145/400] Loss = 2.117278575897217 TrainAccuracy = 0.3125 ValAccuracy = 0.3845\n",
            "Epoch [146/400] Loss = 2.068174362182617 TrainAccuracy = 0.390625 ValAccuracy = 0.3828\n",
            "Epoch [147/400] Loss = 2.054394483566284 TrainAccuracy = 0.390625 ValAccuracy = 0.3828\n",
            "Epoch [148/400] Loss = 2.054574489593506 TrainAccuracy = 0.390625 ValAccuracy = 0.3917\n",
            "Epoch [149/400] Loss = 2.0920724868774414 TrainAccuracy = 0.359375 ValAccuracy = 0.3834\n",
            "Epoch [150/400] Loss = 2.097684144973755 TrainAccuracy = 0.34375 ValAccuracy = 0.3871\n",
            "150 saved\n",
            "Epoch [151/400] Loss = 2.126389503479004 TrainAccuracy = 0.328125 ValAccuracy = 0.3844\n",
            "Epoch [152/400] Loss = 2.1281259059906006 TrainAccuracy = 0.3125 ValAccuracy = 0.3809\n",
            "Epoch [153/400] Loss = 2.1584951877593994 TrainAccuracy = 0.28125 ValAccuracy = 0.3891\n",
            "Epoch [154/400] Loss = 2.085308313369751 TrainAccuracy = 0.359375 ValAccuracy = 0.3828\n",
            "Epoch [155/400] Loss = 2.1202609539031982 TrainAccuracy = 0.328125 ValAccuracy = 0.3881\n",
            "Epoch [156/400] Loss = 2.0264480113983154 TrainAccuracy = 0.453125 ValAccuracy = 0.3896\n",
            "Epoch [157/400] Loss = 2.0459213256835938 TrainAccuracy = 0.421875 ValAccuracy = 0.3836\n",
            "Epoch [158/400] Loss = 2.085650682449341 TrainAccuracy = 0.390625 ValAccuracy = 0.3919\n",
            "Epoch [159/400] Loss = 2.010791778564453 TrainAccuracy = 0.46875 ValAccuracy = 0.3891\n",
            "Epoch [160/400] Loss = 2.025914430618286 TrainAccuracy = 0.40625 ValAccuracy = 0.3963\n",
            "160 saved\n",
            "Epoch [161/400] Loss = 2.025470495223999 TrainAccuracy = 0.453125 ValAccuracy = 0.3898\n",
            "Epoch [162/400] Loss = 1.9912351369857788 TrainAccuracy = 0.484375 ValAccuracy = 0.3872\n",
            "Epoch [163/400] Loss = 1.9996891021728516 TrainAccuracy = 0.484375 ValAccuracy = 0.3904\n",
            "Epoch [164/400] Loss = 1.9945068359375 TrainAccuracy = 0.5 ValAccuracy = 0.3843\n",
            "Epoch [165/400] Loss = 2.0479931831359863 TrainAccuracy = 0.421875 ValAccuracy = 0.3924\n",
            "Epoch [166/400] Loss = 2.1319668292999268 TrainAccuracy = 0.3125 ValAccuracy = 0.3859\n",
            "Epoch [167/400] Loss = 2.038046360015869 TrainAccuracy = 0.421875 ValAccuracy = 0.3936\n",
            "Epoch [168/400] Loss = 2.0730490684509277 TrainAccuracy = 0.375 ValAccuracy = 0.3901\n",
            "Epoch [169/400] Loss = 2.084519624710083 TrainAccuracy = 0.375 ValAccuracy = 0.3957\n",
            "Epoch [170/400] Loss = 2.0517308712005615 TrainAccuracy = 0.40625 ValAccuracy = 0.4008\n",
            "170 saved\n",
            "Epoch [171/400] Loss = 2.0020434856414795 TrainAccuracy = 0.453125 ValAccuracy = 0.3928\n",
            "Epoch [172/400] Loss = 2.085191011428833 TrainAccuracy = 0.375 ValAccuracy = 0.3983\n",
            "Epoch [173/400] Loss = 2.1104581356048584 TrainAccuracy = 0.3125 ValAccuracy = 0.3952\n",
            "Epoch [174/400] Loss = 2.1211001873016357 TrainAccuracy = 0.3125 ValAccuracy = 0.3907\n",
            "Epoch [175/400] Loss = 2.075148105621338 TrainAccuracy = 0.390625 ValAccuracy = 0.3932\n",
            "Epoch [176/400] Loss = 2.069553852081299 TrainAccuracy = 0.375 ValAccuracy = 0.3959\n",
            "Epoch [177/400] Loss = 2.068729877471924 TrainAccuracy = 0.390625 ValAccuracy = 0.3933\n",
            "Epoch [178/400] Loss = 2.049917459487915 TrainAccuracy = 0.40625 ValAccuracy = 0.3946\n",
            "Epoch [179/400] Loss = 2.092586040496826 TrainAccuracy = 0.359375 ValAccuracy = 0.3971\n",
            "Epoch [180/400] Loss = 2.0979363918304443 TrainAccuracy = 0.328125 ValAccuracy = 0.3967\n",
            "180 saved\n",
            "Epoch [181/400] Loss = 2.03694486618042 TrainAccuracy = 0.390625 ValAccuracy = 0.401\n",
            "Epoch [182/400] Loss = 2.04929780960083 TrainAccuracy = 0.40625 ValAccuracy = 0.3995\n",
            "Epoch [183/400] Loss = 2.0922844409942627 TrainAccuracy = 0.359375 ValAccuracy = 0.3989\n",
            "Epoch [184/400] Loss = 2.0483851432800293 TrainAccuracy = 0.40625 ValAccuracy = 0.4\n",
            "Epoch [185/400] Loss = 1.9959392547607422 TrainAccuracy = 0.46875 ValAccuracy = 0.4048\n",
            "Epoch [186/400] Loss = 2.0758628845214844 TrainAccuracy = 0.40625 ValAccuracy = 0.3988\n",
            "Epoch [187/400] Loss = 2.0601205825805664 TrainAccuracy = 0.375 ValAccuracy = 0.4056\n",
            "Epoch [188/400] Loss = 2.0608489513397217 TrainAccuracy = 0.375 ValAccuracy = 0.4003\n",
            "Epoch [189/400] Loss = 2.122694730758667 TrainAccuracy = 0.28125 ValAccuracy = 0.3991\n",
            "Epoch [190/400] Loss = 2.131725311279297 TrainAccuracy = 0.328125 ValAccuracy = 0.3986\n",
            "190 saved\n",
            "Epoch [191/400] Loss = 1.939513087272644 TrainAccuracy = 0.515625 ValAccuracy = 0.4036\n",
            "Epoch [192/400] Loss = 2.059833526611328 TrainAccuracy = 0.390625 ValAccuracy = 0.4019\n",
            "Epoch [193/400] Loss = 2.037963628768921 TrainAccuracy = 0.4375 ValAccuracy = 0.3918\n",
            "Epoch [194/400] Loss = 2.144871711730957 TrainAccuracy = 0.28125 ValAccuracy = 0.4056\n",
            "Epoch [195/400] Loss = 1.9962449073791504 TrainAccuracy = 0.453125 ValAccuracy = 0.3989\n",
            "Epoch [196/400] Loss = 2.0279288291931152 TrainAccuracy = 0.4375 ValAccuracy = 0.4071\n",
            "Epoch [197/400] Loss = 2.092890501022339 TrainAccuracy = 0.359375 ValAccuracy = 0.4084\n",
            "Epoch [198/400] Loss = 2.0756912231445312 TrainAccuracy = 0.375 ValAccuracy = 0.3959\n",
            "Epoch [199/400] Loss = 1.9407167434692383 TrainAccuracy = 0.546875 ValAccuracy = 0.4078\n",
            "Epoch [200/400] Loss = 2.009746551513672 TrainAccuracy = 0.453125 ValAccuracy = 0.4047\n",
            "200 saved\n",
            "Epoch [201/400] Loss = 2.07985520362854 TrainAccuracy = 0.34375 ValAccuracy = 0.4056\n",
            "Epoch [202/400] Loss = 2.017374277114868 TrainAccuracy = 0.46875 ValAccuracy = 0.4086\n",
            "Epoch [203/400] Loss = 2.056328773498535 TrainAccuracy = 0.421875 ValAccuracy = 0.4097\n",
            "Epoch [204/400] Loss = 2.100749969482422 TrainAccuracy = 0.359375 ValAccuracy = 0.407\n",
            "Epoch [205/400] Loss = 1.9964988231658936 TrainAccuracy = 0.453125 ValAccuracy = 0.4136\n",
            "Epoch [206/400] Loss = 2.0027031898498535 TrainAccuracy = 0.46875 ValAccuracy = 0.4131\n",
            "Epoch [207/400] Loss = 2.104330062866211 TrainAccuracy = 0.375 ValAccuracy = 0.4151\n",
            "Epoch [208/400] Loss = 2.0833303928375244 TrainAccuracy = 0.375 ValAccuracy = 0.4131\n",
            "Epoch [209/400] Loss = 2.039147138595581 TrainAccuracy = 0.421875 ValAccuracy = 0.415\n",
            "Epoch [210/400] Loss = 2.111579656600952 TrainAccuracy = 0.328125 ValAccuracy = 0.4083\n",
            "210 saved\n",
            "Epoch [211/400] Loss = 2.120771646499634 TrainAccuracy = 0.3125 ValAccuracy = 0.4056\n",
            "Epoch [212/400] Loss = 2.0449283123016357 TrainAccuracy = 0.421875 ValAccuracy = 0.4134\n",
            "Epoch [213/400] Loss = 2.1168432235717773 TrainAccuracy = 0.34375 ValAccuracy = 0.4017\n",
            "Epoch [214/400] Loss = 2.025167942047119 TrainAccuracy = 0.421875 ValAccuracy = 0.4075\n",
            "Epoch [215/400] Loss = 2.0402138233184814 TrainAccuracy = 0.4375 ValAccuracy = 0.4122\n",
            "Epoch [216/400] Loss = 2.063701868057251 TrainAccuracy = 0.390625 ValAccuracy = 0.4191\n",
            "Epoch [217/400] Loss = 2.09242582321167 TrainAccuracy = 0.359375 ValAccuracy = 0.4146\n",
            "Epoch [218/400] Loss = 2.1264865398406982 TrainAccuracy = 0.359375 ValAccuracy = 0.4214\n",
            "Epoch [219/400] Loss = 1.9689760208129883 TrainAccuracy = 0.515625 ValAccuracy = 0.414\n",
            "Epoch [220/400] Loss = 2.0197970867156982 TrainAccuracy = 0.453125 ValAccuracy = 0.4136\n",
            "220 saved\n",
            "Epoch [221/400] Loss = 1.9412031173706055 TrainAccuracy = 0.53125 ValAccuracy = 0.4171\n",
            "Epoch [222/400] Loss = 2.034684896469116 TrainAccuracy = 0.4375 ValAccuracy = 0.407\n",
            "Epoch [223/400] Loss = 2.1159584522247314 TrainAccuracy = 0.359375 ValAccuracy = 0.4119\n",
            "Epoch [224/400] Loss = 2.0134246349334717 TrainAccuracy = 0.4375 ValAccuracy = 0.4139\n",
            "Epoch [225/400] Loss = 2.0070583820343018 TrainAccuracy = 0.453125 ValAccuracy = 0.4189\n",
            "Epoch [226/400] Loss = 2.0882408618927 TrainAccuracy = 0.359375 ValAccuracy = 0.4152\n",
            "Epoch [227/400] Loss = 2.0335237979888916 TrainAccuracy = 0.390625 ValAccuracy = 0.4217\n",
            "Epoch [228/400] Loss = 2.0131685733795166 TrainAccuracy = 0.46875 ValAccuracy = 0.4173\n",
            "Epoch [229/400] Loss = 2.0966904163360596 TrainAccuracy = 0.359375 ValAccuracy = 0.4135\n",
            "Epoch [230/400] Loss = 1.9543521404266357 TrainAccuracy = 0.5 ValAccuracy = 0.417\n",
            "230 saved\n",
            "Epoch [231/400] Loss = 2.018087387084961 TrainAccuracy = 0.4375 ValAccuracy = 0.4165\n",
            "Epoch [232/400] Loss = 2.122328996658325 TrainAccuracy = 0.34375 ValAccuracy = 0.4196\n",
            "Epoch [233/400] Loss = 2.1103081703186035 TrainAccuracy = 0.328125 ValAccuracy = 0.4229\n",
            "Epoch [234/400] Loss = 2.0228209495544434 TrainAccuracy = 0.421875 ValAccuracy = 0.4206\n",
            "Epoch [235/400] Loss = 1.8650072813034058 TrainAccuracy = 0.625 ValAccuracy = 0.4294\n",
            "Epoch [236/400] Loss = 1.9457836151123047 TrainAccuracy = 0.5 ValAccuracy = 0.4194\n",
            "Epoch [237/400] Loss = 2.0444936752319336 TrainAccuracy = 0.421875 ValAccuracy = 0.4312\n",
            "Epoch [238/400] Loss = 2.041409730911255 TrainAccuracy = 0.40625 ValAccuracy = 0.4162\n",
            "Epoch [239/400] Loss = 2.0614724159240723 TrainAccuracy = 0.390625 ValAccuracy = 0.4259\n",
            "Epoch [240/400] Loss = 2.0099666118621826 TrainAccuracy = 0.4375 ValAccuracy = 0.4145\n",
            "240 saved\n",
            "Epoch [241/400] Loss = 2.080317735671997 TrainAccuracy = 0.375 ValAccuracy = 0.4273\n",
            "Epoch [242/400] Loss = 2.016068458557129 TrainAccuracy = 0.4375 ValAccuracy = 0.4237\n",
            "Epoch [243/400] Loss = 1.9985637664794922 TrainAccuracy = 0.46875 ValAccuracy = 0.4204\n",
            "Epoch [244/400] Loss = 2.1396195888519287 TrainAccuracy = 0.296875 ValAccuracy = 0.4188\n",
            "Epoch [245/400] Loss = 2.0065083503723145 TrainAccuracy = 0.46875 ValAccuracy = 0.4258\n",
            "Epoch [246/400] Loss = 1.9474259614944458 TrainAccuracy = 0.515625 ValAccuracy = 0.4216\n",
            "Epoch [247/400] Loss = 1.963787317276001 TrainAccuracy = 0.53125 ValAccuracy = 0.4204\n",
            "Epoch [248/400] Loss = 2.1001644134521484 TrainAccuracy = 0.359375 ValAccuracy = 0.4274\n",
            "Epoch [249/400] Loss = 2.0978803634643555 TrainAccuracy = 0.34375 ValAccuracy = 0.4239\n",
            "Epoch [250/400] Loss = 2.109220027923584 TrainAccuracy = 0.34375 ValAccuracy = 0.4228\n",
            "250 saved\n",
            "Epoch [251/400] Loss = 2.0442638397216797 TrainAccuracy = 0.40625 ValAccuracy = 0.4297\n",
            "Epoch [252/400] Loss = 1.964953899383545 TrainAccuracy = 0.546875 ValAccuracy = 0.4299\n",
            "Epoch [253/400] Loss = 1.9883370399475098 TrainAccuracy = 0.5 ValAccuracy = 0.4287\n",
            "Epoch [254/400] Loss = 1.971615195274353 TrainAccuracy = 0.46875 ValAccuracy = 0.4248\n",
            "Epoch [255/400] Loss = 1.9488965272903442 TrainAccuracy = 0.515625 ValAccuracy = 0.4303\n",
            "Epoch [256/400] Loss = 2.0087084770202637 TrainAccuracy = 0.484375 ValAccuracy = 0.4302\n",
            "Epoch [257/400] Loss = 2.0152676105499268 TrainAccuracy = 0.40625 ValAccuracy = 0.4286\n",
            "Epoch [258/400] Loss = 2.0242085456848145 TrainAccuracy = 0.46875 ValAccuracy = 0.4271\n",
            "Epoch [259/400] Loss = 1.9739890098571777 TrainAccuracy = 0.46875 ValAccuracy = 0.4282\n",
            "Epoch [260/400] Loss = 1.9700661897659302 TrainAccuracy = 0.484375 ValAccuracy = 0.4362\n",
            "260 saved\n",
            "Epoch [261/400] Loss = 1.9811502695083618 TrainAccuracy = 0.5 ValAccuracy = 0.4353\n",
            "Epoch [262/400] Loss = 2.004988670349121 TrainAccuracy = 0.421875 ValAccuracy = 0.4357\n",
            "Epoch [263/400] Loss = 1.890700101852417 TrainAccuracy = 0.578125 ValAccuracy = 0.4325\n",
            "Epoch [264/400] Loss = 1.9302141666412354 TrainAccuracy = 0.546875 ValAccuracy = 0.4373\n",
            "Epoch [265/400] Loss = 1.9213993549346924 TrainAccuracy = 0.53125 ValAccuracy = 0.4357\n",
            "Epoch [266/400] Loss = 2.044797658920288 TrainAccuracy = 0.421875 ValAccuracy = 0.438\n",
            "Epoch [267/400] Loss = 2.022559881210327 TrainAccuracy = 0.4375 ValAccuracy = 0.4353\n",
            "Epoch [268/400] Loss = 1.926820993423462 TrainAccuracy = 0.515625 ValAccuracy = 0.4346\n",
            "Epoch [269/400] Loss = 2.0218167304992676 TrainAccuracy = 0.4375 ValAccuracy = 0.4343\n",
            "Epoch [270/400] Loss = 2.098071575164795 TrainAccuracy = 0.375 ValAccuracy = 0.4379\n",
            "270 saved\n",
            "Epoch [271/400] Loss = 2.0066540241241455 TrainAccuracy = 0.46875 ValAccuracy = 0.4403\n",
            "Epoch [272/400] Loss = 2.007558822631836 TrainAccuracy = 0.4375 ValAccuracy = 0.4322\n",
            "Epoch [273/400] Loss = 2.0561013221740723 TrainAccuracy = 0.375 ValAccuracy = 0.439\n",
            "Epoch [274/400] Loss = 2.053579807281494 TrainAccuracy = 0.390625 ValAccuracy = 0.4346\n",
            "Epoch [275/400] Loss = 1.9808557033538818 TrainAccuracy = 0.484375 ValAccuracy = 0.435\n",
            "Epoch [276/400] Loss = 2.0013487339019775 TrainAccuracy = 0.453125 ValAccuracy = 0.4382\n",
            "Epoch [277/400] Loss = 2.011251211166382 TrainAccuracy = 0.453125 ValAccuracy = 0.4375\n",
            "Epoch [278/400] Loss = 1.979917049407959 TrainAccuracy = 0.453125 ValAccuracy = 0.4356\n",
            "Epoch [279/400] Loss = 2.122450351715088 TrainAccuracy = 0.328125 ValAccuracy = 0.4348\n",
            "Epoch [280/400] Loss = 2.0102365016937256 TrainAccuracy = 0.453125 ValAccuracy = 0.4369\n",
            "280 saved\n",
            "Epoch [281/400] Loss = 1.9509475231170654 TrainAccuracy = 0.515625 ValAccuracy = 0.4403\n",
            "Epoch [282/400] Loss = 1.971978783607483 TrainAccuracy = 0.484375 ValAccuracy = 0.443\n",
            "Epoch [283/400] Loss = 2.0022549629211426 TrainAccuracy = 0.421875 ValAccuracy = 0.4381\n",
            "Epoch [284/400] Loss = 2.074528932571411 TrainAccuracy = 0.390625 ValAccuracy = 0.4316\n",
            "Epoch [285/400] Loss = 1.963321566581726 TrainAccuracy = 0.5 ValAccuracy = 0.4377\n",
            "Epoch [286/400] Loss = 1.9184144735336304 TrainAccuracy = 0.546875 ValAccuracy = 0.4365\n",
            "Epoch [287/400] Loss = 1.9535372257232666 TrainAccuracy = 0.515625 ValAccuracy = 0.4447\n",
            "Epoch [288/400] Loss = 2.0351293087005615 TrainAccuracy = 0.453125 ValAccuracy = 0.4458\n",
            "Epoch [289/400] Loss = 1.984213948249817 TrainAccuracy = 0.484375 ValAccuracy = 0.443\n",
            "Epoch [290/400] Loss = 1.951957106590271 TrainAccuracy = 0.5 ValAccuracy = 0.4446\n",
            "290 saved\n",
            "Epoch [291/400] Loss = 2.002950668334961 TrainAccuracy = 0.46875 ValAccuracy = 0.4456\n",
            "Epoch [292/400] Loss = 1.9926997423171997 TrainAccuracy = 0.453125 ValAccuracy = 0.4421\n",
            "Epoch [293/400] Loss = 1.976575255393982 TrainAccuracy = 0.5 ValAccuracy = 0.4462\n",
            "Epoch [294/400] Loss = 2.0478458404541016 TrainAccuracy = 0.40625 ValAccuracy = 0.4446\n",
            "Epoch [295/400] Loss = 1.9668740034103394 TrainAccuracy = 0.484375 ValAccuracy = 0.434\n",
            "Epoch [296/400] Loss = 2.0118024349212646 TrainAccuracy = 0.4375 ValAccuracy = 0.4438\n",
            "Epoch [297/400] Loss = 1.9570475816726685 TrainAccuracy = 0.515625 ValAccuracy = 0.4431\n",
            "Epoch [298/400] Loss = 1.9800899028778076 TrainAccuracy = 0.484375 ValAccuracy = 0.4394\n",
            "Epoch [299/400] Loss = 1.957937479019165 TrainAccuracy = 0.484375 ValAccuracy = 0.4525\n",
            "Epoch [300/400] Loss = 1.9685956239700317 TrainAccuracy = 0.5 ValAccuracy = 0.4476\n",
            "300 saved\n",
            "Epoch [301/400] Loss = 1.9137301445007324 TrainAccuracy = 0.546875 ValAccuracy = 0.4473\n",
            "Epoch [302/400] Loss = 1.9885132312774658 TrainAccuracy = 0.46875 ValAccuracy = 0.4516\n",
            "Epoch [303/400] Loss = 2.0362367630004883 TrainAccuracy = 0.4375 ValAccuracy = 0.4375\n",
            "Epoch [304/400] Loss = 2.0340447425842285 TrainAccuracy = 0.4375 ValAccuracy = 0.4462\n",
            "Epoch [305/400] Loss = 2.0444278717041016 TrainAccuracy = 0.40625 ValAccuracy = 0.4497\n",
            "Epoch [306/400] Loss = 1.8275383710861206 TrainAccuracy = 0.65625 ValAccuracy = 0.4506\n",
            "Epoch [307/400] Loss = 2.0421502590179443 TrainAccuracy = 0.390625 ValAccuracy = 0.4482\n",
            "Epoch [308/400] Loss = 2.0812559127807617 TrainAccuracy = 0.359375 ValAccuracy = 0.4405\n",
            "Epoch [309/400] Loss = 2.0541162490844727 TrainAccuracy = 0.40625 ValAccuracy = 0.4448\n",
            "Epoch [310/400] Loss = 1.9795949459075928 TrainAccuracy = 0.5 ValAccuracy = 0.4499\n",
            "310 saved\n",
            "Epoch [311/400] Loss = 1.9617598056793213 TrainAccuracy = 0.46875 ValAccuracy = 0.45\n",
            "Epoch [312/400] Loss = 2.017829179763794 TrainAccuracy = 0.4375 ValAccuracy = 0.4531\n",
            "Epoch [313/400] Loss = 1.9986381530761719 TrainAccuracy = 0.4375 ValAccuracy = 0.4404\n",
            "Epoch [314/400] Loss = 1.9590777158737183 TrainAccuracy = 0.5 ValAccuracy = 0.4559\n",
            "Epoch [315/400] Loss = 1.9379901885986328 TrainAccuracy = 0.53125 ValAccuracy = 0.4447\n",
            "Epoch [316/400] Loss = 2.0228352546691895 TrainAccuracy = 0.484375 ValAccuracy = 0.4465\n",
            "Epoch [317/400] Loss = 2.0166022777557373 TrainAccuracy = 0.4375 ValAccuracy = 0.4535\n",
            "Epoch [318/400] Loss = 2.0038230419158936 TrainAccuracy = 0.421875 ValAccuracy = 0.4525\n",
            "Epoch [319/400] Loss = 1.988942265510559 TrainAccuracy = 0.46875 ValAccuracy = 0.4571\n",
            "Epoch [320/400] Loss = 2.0427918434143066 TrainAccuracy = 0.390625 ValAccuracy = 0.4449\n",
            "320 saved\n",
            "Epoch [321/400] Loss = 1.8979495763778687 TrainAccuracy = 0.5625 ValAccuracy = 0.453\n",
            "Epoch [322/400] Loss = 2.06734037399292 TrainAccuracy = 0.375 ValAccuracy = 0.452\n",
            "Epoch [323/400] Loss = 2.002009868621826 TrainAccuracy = 0.5 ValAccuracy = 0.4511\n",
            "Epoch [324/400] Loss = 2.006190776824951 TrainAccuracy = 0.4375 ValAccuracy = 0.4522\n",
            "Epoch [325/400] Loss = 1.9384207725524902 TrainAccuracy = 0.5 ValAccuracy = 0.4598\n",
            "Epoch [326/400] Loss = 1.9775891304016113 TrainAccuracy = 0.484375 ValAccuracy = 0.4463\n",
            "Epoch [327/400] Loss = 2.0566439628601074 TrainAccuracy = 0.390625 ValAccuracy = 0.4543\n",
            "Epoch [328/400] Loss = 1.988070011138916 TrainAccuracy = 0.46875 ValAccuracy = 0.4502\n",
            "Epoch [329/400] Loss = 2.011265277862549 TrainAccuracy = 0.453125 ValAccuracy = 0.4524\n",
            "Epoch [330/400] Loss = 2.0284197330474854 TrainAccuracy = 0.4375 ValAccuracy = 0.4454\n",
            "330 saved\n",
            "Epoch [331/400] Loss = 2.031651258468628 TrainAccuracy = 0.390625 ValAccuracy = 0.4554\n",
            "Epoch [332/400] Loss = 1.9146816730499268 TrainAccuracy = 0.5625 ValAccuracy = 0.4556\n",
            "Epoch [333/400] Loss = 1.9937551021575928 TrainAccuracy = 0.484375 ValAccuracy = 0.4573\n",
            "Epoch [334/400] Loss = 2.0183863639831543 TrainAccuracy = 0.421875 ValAccuracy = 0.4468\n",
            "Epoch [335/400] Loss = 2.051621913909912 TrainAccuracy = 0.375 ValAccuracy = 0.4654\n",
            "Epoch [336/400] Loss = 1.9781428575515747 TrainAccuracy = 0.484375 ValAccuracy = 0.4623\n",
            "Epoch [337/400] Loss = 2.029001474380493 TrainAccuracy = 0.4375 ValAccuracy = 0.4546\n",
            "Epoch [338/400] Loss = 2.0567898750305176 TrainAccuracy = 0.40625 ValAccuracy = 0.4554\n",
            "Epoch [339/400] Loss = 1.99288010597229 TrainAccuracy = 0.484375 ValAccuracy = 0.4354\n",
            "Epoch [340/400] Loss = 2.023047924041748 TrainAccuracy = 0.4375 ValAccuracy = 0.4584\n",
            "340 saved\n",
            "Epoch [341/400] Loss = 2.017263412475586 TrainAccuracy = 0.390625 ValAccuracy = 0.4543\n",
            "Epoch [342/400] Loss = 1.9918770790100098 TrainAccuracy = 0.453125 ValAccuracy = 0.4583\n",
            "Epoch [343/400] Loss = 1.988677740097046 TrainAccuracy = 0.46875 ValAccuracy = 0.4476\n",
            "Epoch [344/400] Loss = 1.997733235359192 TrainAccuracy = 0.4375 ValAccuracy = 0.4571\n",
            "Epoch [345/400] Loss = 1.94901704788208 TrainAccuracy = 0.515625 ValAccuracy = 0.451\n",
            "Epoch [346/400] Loss = 1.9796098470687866 TrainAccuracy = 0.46875 ValAccuracy = 0.461\n",
            "Epoch [347/400] Loss = 1.9807618856430054 TrainAccuracy = 0.484375 ValAccuracy = 0.4516\n",
            "Epoch [348/400] Loss = 1.9698108434677124 TrainAccuracy = 0.5 ValAccuracy = 0.4598\n",
            "Epoch [349/400] Loss = 1.9661511182785034 TrainAccuracy = 0.515625 ValAccuracy = 0.4609\n",
            "Epoch [350/400] Loss = 2.0345592498779297 TrainAccuracy = 0.421875 ValAccuracy = 0.4551\n",
            "350 saved\n",
            "Epoch [351/400] Loss = 2.007995128631592 TrainAccuracy = 0.453125 ValAccuracy = 0.462\n",
            "Epoch [352/400] Loss = 1.9484080076217651 TrainAccuracy = 0.484375 ValAccuracy = 0.4638\n",
            "Epoch [353/400] Loss = 2.0188148021698 TrainAccuracy = 0.421875 ValAccuracy = 0.463\n",
            "Epoch [354/400] Loss = 2.0036704540252686 TrainAccuracy = 0.453125 ValAccuracy = 0.4645\n",
            "Epoch [355/400] Loss = 2.067725658416748 TrainAccuracy = 0.390625 ValAccuracy = 0.4627\n",
            "Epoch [356/400] Loss = 1.968124270439148 TrainAccuracy = 0.484375 ValAccuracy = 0.454\n",
            "Epoch [357/400] Loss = 1.9726569652557373 TrainAccuracy = 0.484375 ValAccuracy = 0.4652\n",
            "Epoch [358/400] Loss = 2.0561444759368896 TrainAccuracy = 0.421875 ValAccuracy = 0.4703\n",
            "Epoch [359/400] Loss = 2.0169453620910645 TrainAccuracy = 0.4375 ValAccuracy = 0.4616\n",
            "Epoch [360/400] Loss = 1.9896576404571533 TrainAccuracy = 0.46875 ValAccuracy = 0.4643\n",
            "360 saved\n",
            "Epoch [361/400] Loss = 1.9720438718795776 TrainAccuracy = 0.484375 ValAccuracy = 0.4679\n",
            "Epoch [362/400] Loss = 2.017008066177368 TrainAccuracy = 0.46875 ValAccuracy = 0.459\n",
            "Epoch [363/400] Loss = 1.9717265367507935 TrainAccuracy = 0.5 ValAccuracy = 0.4649\n",
            "Epoch [364/400] Loss = 1.956506609916687 TrainAccuracy = 0.5 ValAccuracy = 0.462\n",
            "Epoch [365/400] Loss = 2.0289254188537598 TrainAccuracy = 0.4375 ValAccuracy = 0.4643\n",
            "Epoch [366/400] Loss = 2.022213935852051 TrainAccuracy = 0.4375 ValAccuracy = 0.4587\n",
            "Epoch [367/400] Loss = 1.91292142868042 TrainAccuracy = 0.546875 ValAccuracy = 0.4663\n",
            "Epoch [368/400] Loss = 2.000025749206543 TrainAccuracy = 0.453125 ValAccuracy = 0.4593\n",
            "Epoch [369/400] Loss = 1.975856900215149 TrainAccuracy = 0.46875 ValAccuracy = 0.469\n",
            "Epoch [370/400] Loss = 1.9347107410430908 TrainAccuracy = 0.546875 ValAccuracy = 0.4655\n",
            "370 saved\n",
            "Epoch [371/400] Loss = 1.9436501264572144 TrainAccuracy = 0.5 ValAccuracy = 0.4575\n",
            "Epoch [372/400] Loss = 2.15368914604187 TrainAccuracy = 0.3125 ValAccuracy = 0.4663\n",
            "Epoch [373/400] Loss = 1.902506947517395 TrainAccuracy = 0.5625 ValAccuracy = 0.4609\n",
            "Epoch [374/400] Loss = 1.95269775390625 TrainAccuracy = 0.5 ValAccuracy = 0.4632\n",
            "Epoch [375/400] Loss = 2.0407629013061523 TrainAccuracy = 0.40625 ValAccuracy = 0.4674\n",
            "Epoch [376/400] Loss = 2.0145153999328613 TrainAccuracy = 0.421875 ValAccuracy = 0.4662\n",
            "Epoch [377/400] Loss = 2.0714149475097656 TrainAccuracy = 0.359375 ValAccuracy = 0.4633\n",
            "Epoch [378/400] Loss = 1.9254381656646729 TrainAccuracy = 0.546875 ValAccuracy = 0.4652\n",
            "Epoch [379/400] Loss = 2.0080854892730713 TrainAccuracy = 0.4375 ValAccuracy = 0.4583\n",
            "Epoch [380/400] Loss = 1.9792896509170532 TrainAccuracy = 0.46875 ValAccuracy = 0.4582\n",
            "380 saved\n",
            "Epoch [381/400] Loss = 2.068910598754883 TrainAccuracy = 0.375 ValAccuracy = 0.4664\n",
            "Epoch [382/400] Loss = 1.968037486076355 TrainAccuracy = 0.46875 ValAccuracy = 0.4701\n",
            "Epoch [383/400] Loss = 1.9826256036758423 TrainAccuracy = 0.484375 ValAccuracy = 0.4628\n",
            "Epoch [384/400] Loss = 2.0661122798919678 TrainAccuracy = 0.375 ValAccuracy = 0.4655\n",
            "Epoch [385/400] Loss = 2.0019001960754395 TrainAccuracy = 0.46875 ValAccuracy = 0.4718\n",
            "Epoch [386/400] Loss = 1.8947392702102661 TrainAccuracy = 0.578125 ValAccuracy = 0.4733\n",
            "Epoch [387/400] Loss = 1.9072967767715454 TrainAccuracy = 0.5625 ValAccuracy = 0.4671\n",
            "Epoch [388/400] Loss = 1.8530980348587036 TrainAccuracy = 0.609375 ValAccuracy = 0.4667\n",
            "Epoch [389/400] Loss = 1.965455412864685 TrainAccuracy = 0.5 ValAccuracy = 0.4698\n",
            "Epoch [390/400] Loss = 1.9649393558502197 TrainAccuracy = 0.484375 ValAccuracy = 0.4631\n",
            "390 saved\n",
            "Epoch [391/400] Loss = 1.8562768697738647 TrainAccuracy = 0.625 ValAccuracy = 0.4736\n",
            "Epoch [392/400] Loss = 1.9583103656768799 TrainAccuracy = 0.484375 ValAccuracy = 0.4657\n",
            "Epoch [393/400] Loss = 2.0354015827178955 TrainAccuracy = 0.40625 ValAccuracy = 0.4676\n",
            "Epoch [394/400] Loss = 1.9744062423706055 TrainAccuracy = 0.515625 ValAccuracy = 0.4673\n",
            "Epoch [395/400] Loss = 2.0085325241088867 TrainAccuracy = 0.421875 ValAccuracy = 0.4671\n",
            "Epoch [396/400] Loss = 1.9499388933181763 TrainAccuracy = 0.53125 ValAccuracy = 0.471\n",
            "Epoch [397/400] Loss = 2.0142245292663574 TrainAccuracy = 0.4375 ValAccuracy = 0.4753\n",
            "Epoch [398/400] Loss = 1.9549574851989746 TrainAccuracy = 0.53125 ValAccuracy = 0.4759\n",
            "Epoch [399/400] Loss = 1.8736540079116821 TrainAccuracy = 0.578125 ValAccuracy = 0.476\n",
            "Epoch [400/400] Loss = 1.954002857208252 TrainAccuracy = 0.515625 ValAccuracy = 0.4677\n",
            "400 saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoF1fI2U3JX_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5b541666-454d-4e83-ac8e-2966bcdc28ad"
      },
      "source": [
        "import numpy as np\n",
        "x = torch.load(\"/content/drive/My Drive/Artificial intelligence/bigmodel.th\")\n",
        "binnet = BinResNet(BinBlock, [3, 3, 3], 10, bin_fn=binarize).to(device)\n",
        "binnet.load_state_dict(x[\"binnet_state_dict\"])\n",
        "\n",
        "with torch.no_grad():\n",
        "    losses = []\n",
        "    x = 0\n",
        "    for i, (images, labels) in enumerate(test_loader):\n",
        "        # print(i)\n",
        "        images = images.to(device)\n",
        "        # print(images.shape)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        out = binnet(images)\n",
        "        ce_loss = nn.CrossEntropyLoss()(out, labels)\n",
        "        losses.append(ce_loss.item())\n",
        "        x += torch.sum(labels==torch.argmax(out, dim=-1)).to(\"cpu\").item()\n",
        "    print(f\"loss {np.mean(losses)} TestAccuracy {x/10000}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss 1.9883591887317127 TestAccuracy 0.4699\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myQDjhFlWDQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mapbinnet = BinResNet(BinBlock, [3, 3, 3], 10, bin_fn=\"map\").to(device)\n",
        "x = torch.load(\"/content/drive/My Drive/Artificial intelligence/bigmodel.th\")\n",
        "net = ResNet(BinBlock, [3, 3, 3], 10).to(device)\n",
        "net.load_state_dict(x[\"net_state_dict\"])\n",
        "mapbinnet_optim = torch.optim.SGD(mapbinnet.parameters(), lr = 0.1, momentum=0.9)\n",
        "# Copy the full precision weights\n",
        "for (name_p, p), (name_q, q) in zip(net.named_parameters(), mapbinnet.named_parameters()):\n",
        "    if name_p==name_q:\n",
        "        q.data = p.data"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rcQF21IdOSy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ba3af20a-d1c1-4454-f2d9-c0fb58bfc185"
      },
      "source": [
        "# Some warm up for the mapping conv weights\n",
        "num_warmup_epochs = 400\n",
        "for epoch in range(num_warmup_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        out = mapbinnet(images)\n",
        "        net_optim.zero_grad()\n",
        "        mapbinnet_optim.zero_grad()\n",
        "\n",
        "        noisy_loss, mapbinne, grad_aux_W = learn(net, mapbinnet)\n",
        "        ce_loss = nn.CrossEntropyLoss()(out, labels)\n",
        "        ce_loss.backward()\n",
        "        nn.utils.clip_grad_value_(mapbinnet.parameters(), 1)\n",
        "\n",
        "        for (name_p, p), (name_q, q) in zip(net.named_parameters(), mapbinnet.named_parameters()):\n",
        "            if name_p==name_q:\n",
        "                if q.grad is not None:\n",
        "                    p.grad = q.grad\n",
        "                    if all([i in name_p for i in bin]):\n",
        "                        p.grad.data += grad_aux_W[name_p]\n",
        "\n",
        "        net_optim.step()\n",
        "        mapbinnet_optim.step()\n",
        "\n",
        "        # Changing the value of the other parameters to the saved ones\n",
        "        for (name_p, p), (name_q, q) in zip(net.named_parameters(), mapbinnet.named_parameters()):\n",
        "            if name_p == name_q:\n",
        "                q.data = p.data\n",
        "        \n",
        "        trainaccuracy = torch.sum(labels==torch.argmax(out, dim=-1)).to(\"cpu\").item()/len(images)\n",
        "\n",
        "    with torch.no_grad():\n",
        "            x = torch.tensor([0.0], device = device)\n",
        "            for images, labels in val_loader:\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                # print(labels.shape)\n",
        "                out = mapbinnet(images)\n",
        "                # print(torch.argmax(out, dim = -1).shape)\n",
        "                x += torch.sum(labels == torch.argmax(out, dim = -1))\n",
        "            valaccuracy = x.to(\"cpu\").item()/10000\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_warmup_epochs}] Loss = {ce_loss.item()} TrainAccuracy = {trainaccuracy} ValAccuracy = {valaccuracy}\")\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        torch.save({\"epoch\": epoch+1, \"net_state_dict\": net.state_dict(), \"mapbinnet_state_dict\": mapbinnet.state_dict()}, \"/content/drive/My Drive/Artificial intelligence/bigmap.th\")\n",
        "        print(f\"{epoch+1} saved\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/400] Loss = 2.282268762588501 TrainAccuracy = 0.125 ValAccuracy = 0.102\n",
            "Epoch [2/400] Loss = 2.282072067260742 TrainAccuracy = 0.140625 ValAccuracy = 0.102\n",
            "Epoch [3/400] Loss = 2.311079502105713 TrainAccuracy = 0.125 ValAccuracy = 0.1093\n",
            "Epoch [4/400] Loss = 2.3521077632904053 TrainAccuracy = 0.0625 ValAccuracy = 0.1139\n",
            "Epoch [5/400] Loss = 2.3116023540496826 TrainAccuracy = 0.140625 ValAccuracy = 0.1082\n",
            "Epoch [6/400] Loss = 2.300687313079834 TrainAccuracy = 0.171875 ValAccuracy = 0.1399\n",
            "Epoch [7/400] Loss = 2.314526319503784 TrainAccuracy = 0.125 ValAccuracy = 0.1505\n",
            "Epoch [8/400] Loss = 2.3149054050445557 TrainAccuracy = 0.109375 ValAccuracy = 0.1655\n",
            "Epoch [9/400] Loss = 2.249946117401123 TrainAccuracy = 0.171875 ValAccuracy = 0.1676\n",
            "Epoch [10/400] Loss = 2.335204839706421 TrainAccuracy = 0.09375 ValAccuracy = 0.1707\n",
            "10 saved\n",
            "Epoch [11/400] Loss = 2.2633652687072754 TrainAccuracy = 0.1875 ValAccuracy = 0.1762\n",
            "Epoch [12/400] Loss = 2.240863084793091 TrainAccuracy = 0.171875 ValAccuracy = 0.1831\n",
            "Epoch [13/400] Loss = 2.244605302810669 TrainAccuracy = 0.1875 ValAccuracy = 0.1836\n",
            "Epoch [14/400] Loss = 2.3006110191345215 TrainAccuracy = 0.125 ValAccuracy = 0.1823\n",
            "Epoch [15/400] Loss = 2.2603845596313477 TrainAccuracy = 0.1875 ValAccuracy = 0.1864\n",
            "Epoch [16/400] Loss = 2.2684528827667236 TrainAccuracy = 0.15625 ValAccuracy = 0.1864\n",
            "Epoch [17/400] Loss = 2.248392343521118 TrainAccuracy = 0.21875 ValAccuracy = 0.1856\n",
            "Epoch [18/400] Loss = 2.3105554580688477 TrainAccuracy = 0.125 ValAccuracy = 0.1976\n",
            "Epoch [19/400] Loss = 2.2548937797546387 TrainAccuracy = 0.15625 ValAccuracy = 0.2004\n",
            "Epoch [20/400] Loss = 2.2008094787597656 TrainAccuracy = 0.25 ValAccuracy = 0.1922\n",
            "20 saved\n",
            "Epoch [21/400] Loss = 2.232120990753174 TrainAccuracy = 0.203125 ValAccuracy = 0.1836\n",
            "Epoch [22/400] Loss = 2.221175193786621 TrainAccuracy = 0.234375 ValAccuracy = 0.1994\n",
            "Epoch [23/400] Loss = 2.231062412261963 TrainAccuracy = 0.234375 ValAccuracy = 0.1996\n",
            "Epoch [24/400] Loss = 2.212526798248291 TrainAccuracy = 0.21875 ValAccuracy = 0.2009\n",
            "Epoch [25/400] Loss = 2.215203285217285 TrainAccuracy = 0.234375 ValAccuracy = 0.1925\n",
            "Epoch [26/400] Loss = 2.286029577255249 TrainAccuracy = 0.171875 ValAccuracy = 0.1995\n",
            "Epoch [27/400] Loss = 2.2021923065185547 TrainAccuracy = 0.296875 ValAccuracy = 0.202\n",
            "Epoch [28/400] Loss = 2.2684483528137207 TrainAccuracy = 0.171875 ValAccuracy = 0.1984\n",
            "Epoch [29/400] Loss = 2.2160778045654297 TrainAccuracy = 0.234375 ValAccuracy = 0.2006\n",
            "Epoch [30/400] Loss = 2.2060399055480957 TrainAccuracy = 0.21875 ValAccuracy = 0.2092\n",
            "30 saved\n",
            "Epoch [31/400] Loss = 2.251572847366333 TrainAccuracy = 0.171875 ValAccuracy = 0.2098\n",
            "Epoch [32/400] Loss = 2.1704025268554688 TrainAccuracy = 0.28125 ValAccuracy = 0.2095\n",
            "Epoch [33/400] Loss = 2.3149702548980713 TrainAccuracy = 0.09375 ValAccuracy = 0.2046\n",
            "Epoch [34/400] Loss = 2.23689341545105 TrainAccuracy = 0.21875 ValAccuracy = 0.1994\n",
            "Epoch [35/400] Loss = 2.1933035850524902 TrainAccuracy = 0.265625 ValAccuracy = 0.2094\n",
            "Epoch [36/400] Loss = 2.2503886222839355 TrainAccuracy = 0.1875 ValAccuracy = 0.2021\n",
            "Epoch [37/400] Loss = 2.2351503372192383 TrainAccuracy = 0.21875 ValAccuracy = 0.2111\n",
            "Epoch [38/400] Loss = 2.2410576343536377 TrainAccuracy = 0.15625 ValAccuracy = 0.2121\n",
            "Epoch [39/400] Loss = 2.2037041187286377 TrainAccuracy = 0.25 ValAccuracy = 0.2021\n",
            "Epoch [40/400] Loss = 2.2457470893859863 TrainAccuracy = 0.1875 ValAccuracy = 0.2111\n",
            "40 saved\n",
            "Epoch [41/400] Loss = 2.215001344680786 TrainAccuracy = 0.21875 ValAccuracy = 0.2115\n",
            "Epoch [42/400] Loss = 2.189575433731079 TrainAccuracy = 0.21875 ValAccuracy = 0.2078\n",
            "Epoch [43/400] Loss = 2.2660584449768066 TrainAccuracy = 0.15625 ValAccuracy = 0.2168\n",
            "Epoch [44/400] Loss = 2.167344570159912 TrainAccuracy = 0.28125 ValAccuracy = 0.2162\n",
            "Epoch [45/400] Loss = 2.1466004848480225 TrainAccuracy = 0.328125 ValAccuracy = 0.2075\n",
            "Epoch [46/400] Loss = 2.227832078933716 TrainAccuracy = 0.203125 ValAccuracy = 0.2171\n",
            "Epoch [47/400] Loss = 2.193445920944214 TrainAccuracy = 0.25 ValAccuracy = 0.2156\n",
            "Epoch [48/400] Loss = 2.2453060150146484 TrainAccuracy = 0.171875 ValAccuracy = 0.2146\n",
            "Epoch [49/400] Loss = 2.2372844219207764 TrainAccuracy = 0.21875 ValAccuracy = 0.21\n",
            "Epoch [50/400] Loss = 2.2173233032226562 TrainAccuracy = 0.1875 ValAccuracy = 0.2135\n",
            "50 saved\n",
            "Epoch [51/400] Loss = 2.295381784439087 TrainAccuracy = 0.140625 ValAccuracy = 0.2115\n",
            "Epoch [52/400] Loss = 2.229118585586548 TrainAccuracy = 0.21875 ValAccuracy = 0.2124\n",
            "Epoch [53/400] Loss = 2.232404947280884 TrainAccuracy = 0.21875 ValAccuracy = 0.214\n",
            "Epoch [54/400] Loss = 2.2096259593963623 TrainAccuracy = 0.25 ValAccuracy = 0.2218\n",
            "Epoch [55/400] Loss = 2.2588894367218018 TrainAccuracy = 0.140625 ValAccuracy = 0.2178\n",
            "Epoch [56/400] Loss = 2.182803153991699 TrainAccuracy = 0.25 ValAccuracy = 0.2055\n",
            "Epoch [57/400] Loss = 2.1793298721313477 TrainAccuracy = 0.34375 ValAccuracy = 0.2093\n",
            "Epoch [58/400] Loss = 2.1560750007629395 TrainAccuracy = 0.296875 ValAccuracy = 0.2119\n",
            "Epoch [59/400] Loss = 2.266414165496826 TrainAccuracy = 0.171875 ValAccuracy = 0.2196\n",
            "Epoch [60/400] Loss = 2.2095696926116943 TrainAccuracy = 0.234375 ValAccuracy = 0.2102\n",
            "60 saved\n",
            "Epoch [61/400] Loss = 2.2141261100769043 TrainAccuracy = 0.203125 ValAccuracy = 0.211\n",
            "Epoch [62/400] Loss = 2.2336881160736084 TrainAccuracy = 0.203125 ValAccuracy = 0.2228\n",
            "Epoch [63/400] Loss = 2.208803653717041 TrainAccuracy = 0.21875 ValAccuracy = 0.2122\n",
            "Epoch [64/400] Loss = 2.3015079498291016 TrainAccuracy = 0.15625 ValAccuracy = 0.2226\n",
            "Epoch [65/400] Loss = 2.2523927688598633 TrainAccuracy = 0.1875 ValAccuracy = 0.2087\n",
            "Epoch [66/400] Loss = 2.182453155517578 TrainAccuracy = 0.28125 ValAccuracy = 0.2154\n",
            "Epoch [67/400] Loss = 2.1950390338897705 TrainAccuracy = 0.25 ValAccuracy = 0.2124\n",
            "Epoch [68/400] Loss = 2.252624034881592 TrainAccuracy = 0.21875 ValAccuracy = 0.2147\n",
            "Epoch [69/400] Loss = 2.251038074493408 TrainAccuracy = 0.125 ValAccuracy = 0.2198\n",
            "Epoch [70/400] Loss = 2.22241473197937 TrainAccuracy = 0.21875 ValAccuracy = 0.2122\n",
            "70 saved\n",
            "Epoch [71/400] Loss = 2.256936550140381 TrainAccuracy = 0.171875 ValAccuracy = 0.2157\n",
            "Epoch [72/400] Loss = 2.246319532394409 TrainAccuracy = 0.171875 ValAccuracy = 0.2192\n",
            "Epoch [73/400] Loss = 2.245086908340454 TrainAccuracy = 0.171875 ValAccuracy = 0.2174\n",
            "Epoch [74/400] Loss = 2.201023817062378 TrainAccuracy = 0.25 ValAccuracy = 0.2258\n",
            "Epoch [75/400] Loss = 2.230379819869995 TrainAccuracy = 0.203125 ValAccuracy = 0.225\n",
            "Epoch [76/400] Loss = 2.2053496837615967 TrainAccuracy = 0.234375 ValAccuracy = 0.2156\n",
            "Epoch [77/400] Loss = 2.2004497051239014 TrainAccuracy = 0.265625 ValAccuracy = 0.2175\n",
            "Epoch [78/400] Loss = 2.2096338272094727 TrainAccuracy = 0.25 ValAccuracy = 0.21\n",
            "Epoch [79/400] Loss = 2.1395373344421387 TrainAccuracy = 0.3125 ValAccuracy = 0.2211\n",
            "Epoch [80/400] Loss = 2.206791877746582 TrainAccuracy = 0.25 ValAccuracy = 0.2151\n",
            "80 saved\n",
            "Epoch [81/400] Loss = 2.2563984394073486 TrainAccuracy = 0.15625 ValAccuracy = 0.2178\n",
            "Epoch [82/400] Loss = 2.270495653152466 TrainAccuracy = 0.171875 ValAccuracy = 0.2226\n",
            "Epoch [83/400] Loss = 2.251260280609131 TrainAccuracy = 0.1875 ValAccuracy = 0.2174\n",
            "Epoch [84/400] Loss = 2.167794942855835 TrainAccuracy = 0.28125 ValAccuracy = 0.224\n",
            "Epoch [85/400] Loss = 2.234593629837036 TrainAccuracy = 0.21875 ValAccuracy = 0.2258\n",
            "Epoch [86/400] Loss = 2.1452476978302 TrainAccuracy = 0.296875 ValAccuracy = 0.2226\n",
            "Epoch [87/400] Loss = 2.2366669178009033 TrainAccuracy = 0.1875 ValAccuracy = 0.2171\n",
            "Epoch [88/400] Loss = 2.2373647689819336 TrainAccuracy = 0.234375 ValAccuracy = 0.2205\n",
            "Epoch [89/400] Loss = 2.2737843990325928 TrainAccuracy = 0.15625 ValAccuracy = 0.2239\n",
            "Epoch [90/400] Loss = 2.223560333251953 TrainAccuracy = 0.265625 ValAccuracy = 0.2157\n",
            "90 saved\n",
            "Epoch [91/400] Loss = 2.1852240562438965 TrainAccuracy = 0.328125 ValAccuracy = 0.2187\n",
            "Epoch [92/400] Loss = 2.2931246757507324 TrainAccuracy = 0.109375 ValAccuracy = 0.2195\n",
            "Epoch [93/400] Loss = 2.211009979248047 TrainAccuracy = 0.21875 ValAccuracy = 0.2181\n",
            "Epoch [94/400] Loss = 2.245495557785034 TrainAccuracy = 0.203125 ValAccuracy = 0.2189\n",
            "Epoch [95/400] Loss = 2.2329564094543457 TrainAccuracy = 0.203125 ValAccuracy = 0.2226\n",
            "Epoch [96/400] Loss = 2.1999826431274414 TrainAccuracy = 0.234375 ValAccuracy = 0.2176\n",
            "Epoch [97/400] Loss = 2.2421162128448486 TrainAccuracy = 0.21875 ValAccuracy = 0.2211\n",
            "Epoch [98/400] Loss = 2.212740898132324 TrainAccuracy = 0.21875 ValAccuracy = 0.2215\n",
            "Epoch [99/400] Loss = 2.195417881011963 TrainAccuracy = 0.296875 ValAccuracy = 0.223\n",
            "Epoch [100/400] Loss = 2.199632406234741 TrainAccuracy = 0.3125 ValAccuracy = 0.2271\n",
            "100 saved\n",
            "Epoch [101/400] Loss = 2.250330924987793 TrainAccuracy = 0.203125 ValAccuracy = 0.2244\n",
            "Epoch [102/400] Loss = 2.242473602294922 TrainAccuracy = 0.1875 ValAccuracy = 0.2171\n",
            "Epoch [103/400] Loss = 2.2133941650390625 TrainAccuracy = 0.234375 ValAccuracy = 0.2241\n",
            "Epoch [104/400] Loss = 2.209099531173706 TrainAccuracy = 0.234375 ValAccuracy = 0.2217\n",
            "Epoch [105/400] Loss = 2.238802194595337 TrainAccuracy = 0.1875 ValAccuracy = 0.2233\n",
            "Epoch [106/400] Loss = 2.2368505001068115 TrainAccuracy = 0.1875 ValAccuracy = 0.2235\n",
            "Epoch [107/400] Loss = 2.248988151550293 TrainAccuracy = 0.203125 ValAccuracy = 0.2155\n",
            "Epoch [108/400] Loss = 2.256199836730957 TrainAccuracy = 0.15625 ValAccuracy = 0.2191\n",
            "Epoch [109/400] Loss = 2.255148410797119 TrainAccuracy = 0.1875 ValAccuracy = 0.2237\n",
            "Epoch [110/400] Loss = 2.187849521636963 TrainAccuracy = 0.25 ValAccuracy = 0.2148\n",
            "110 saved\n",
            "Epoch [111/400] Loss = 2.206728219985962 TrainAccuracy = 0.234375 ValAccuracy = 0.222\n",
            "Epoch [112/400] Loss = 2.341709613800049 TrainAccuracy = 0.078125 ValAccuracy = 0.2134\n",
            "Epoch [113/400] Loss = 2.2558295726776123 TrainAccuracy = 0.1875 ValAccuracy = 0.2164\n",
            "Epoch [114/400] Loss = 2.2269318103790283 TrainAccuracy = 0.203125 ValAccuracy = 0.2188\n",
            "Epoch [115/400] Loss = 2.1298224925994873 TrainAccuracy = 0.328125 ValAccuracy = 0.2253\n",
            "Epoch [116/400] Loss = 2.216928720474243 TrainAccuracy = 0.1875 ValAccuracy = 0.2186\n",
            "Epoch [117/400] Loss = 2.2110188007354736 TrainAccuracy = 0.1875 ValAccuracy = 0.2219\n",
            "Epoch [118/400] Loss = 2.2395918369293213 TrainAccuracy = 0.1875 ValAccuracy = 0.2206\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnF85vR-jQvs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b11b2526-78ac-4070-b762-f7d484c74565"
      },
      "source": [
        "y = torch.rand(3, 3, 224, 224).to(device)\n",
        "z = mapbinnet(y)\n",
        "print(z.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50R9WDlrinZh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a41d641e-65a7-4407-c5f2-74656d8bcefc"
      },
      "source": [
        "# if not trained till 400 epochs\n",
        "mapbinnet_optim = torch.optim.SGD(mapbinnet.parameters(), lr = 0.1, momentum=0.9)\n",
        "x = torch.load(\"/content/drive/My Drive/Artificial intelligence/bigmap.th\")\n",
        "net.load_state_dict(x[\"net_state_dict\"])\n",
        "mapbinnet.load_state_dict(x[\"mapbinnet_state_dict\"])\n",
        "num_warmup_epochs = 400\n",
        "for epoch in range(x[\"epoch\"], num_warmup_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        out = mapbinnet(images)\n",
        "        net_optim.zero_grad()\n",
        "        mapbinnet_optim.zero_grad()\n",
        "\n",
        "        noisy_loss, mapbinne, grad_aux_W = learn(net, mapbinnet)\n",
        "        ce_loss = nn.CrossEntropyLoss()(out, labels)\n",
        "        ce_loss.backward()\n",
        "        nn.utils.clip_grad_value_(mapbinnet.parameters(), 1)\n",
        "\n",
        "        for (name_p, p), (name_q, q) in zip(net.named_parameters(), mapbinnet.named_parameters()):\n",
        "            if name_p==name_q:\n",
        "                if q.grad is not None:\n",
        "                    p.grad = q.grad\n",
        "                    if all([i in name_p for i in bin]):\n",
        "                        p.grad.data += grad_aux_W[name_p]\n",
        "\n",
        "        net_optim.step()\n",
        "        mapbinnet_optim.step()\n",
        "\n",
        "        # Changing the value of the other parameters to the saved ones\n",
        "        for (name_p, p), (name_q, q) in zip(net.named_parameters(), mapbinnet.named_parameters()):\n",
        "            if name_p == name_q:\n",
        "                q.data = p.data\n",
        "        \n",
        "        trainaccuracy = torch.sum(labels==torch.argmax(out, dim=-1)).to(\"cpu\").item()/len(images)\n",
        "\n",
        "    with torch.no_grad():\n",
        "            x = torch.tensor([0.0], device = device)\n",
        "            for images, labels in val_loader:\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                # print(labels.shape)\n",
        "                out = mapbinnet(images)\n",
        "                # print(torch.argmax(out, dim = -1).shape)\n",
        "                x += torch.sum(labels == torch.argmax(out, dim = -1))\n",
        "            valaccuracy = x.to(\"cpu\").item()/10000\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_warmup_epochs}] Loss = {ce_loss.item()} TrainAccuracy = {trainaccuracy} ValAccuracy = {valaccuracy}\")\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        torch.save({\"epoch\": epoch+1, \"net_state_dict\": net.state_dict(), \"mapbinnet_state_dict\": mapbinnet.state_dict()}, \"/content/drive/My Drive/Artificial intelligence/bigmap.th\")\n",
        "        print(f\"{epoch+1} saved\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [141/400] Loss = 2.224152088165283 TrainAccuracy = 0.21875 ValAccuracy = 0.2316\n",
            "Epoch [142/400] Loss = 2.254455089569092 TrainAccuracy = 0.1875 ValAccuracy = 0.2192\n",
            "Epoch [143/400] Loss = 2.196929693222046 TrainAccuracy = 0.203125 ValAccuracy = 0.2235\n",
            "Epoch [144/400] Loss = 2.193085193634033 TrainAccuracy = 0.25 ValAccuracy = 0.2196\n",
            "Epoch [145/400] Loss = 2.2387006282806396 TrainAccuracy = 0.21875 ValAccuracy = 0.2198\n",
            "Epoch [146/400] Loss = 2.2287356853485107 TrainAccuracy = 0.1875 ValAccuracy = 0.2208\n",
            "Epoch [147/400] Loss = 2.2239320278167725 TrainAccuracy = 0.21875 ValAccuracy = 0.2189\n",
            "Epoch [148/400] Loss = 2.2119691371917725 TrainAccuracy = 0.203125 ValAccuracy = 0.2231\n",
            "Epoch [149/400] Loss = 2.1483683586120605 TrainAccuracy = 0.328125 ValAccuracy = 0.2181\n",
            "Epoch [150/400] Loss = 2.262892484664917 TrainAccuracy = 0.125 ValAccuracy = 0.2166\n",
            "150 saved\n",
            "Epoch [151/400] Loss = 2.2370314598083496 TrainAccuracy = 0.21875 ValAccuracy = 0.2187\n",
            "Epoch [152/400] Loss = 2.2068018913269043 TrainAccuracy = 0.28125 ValAccuracy = 0.2198\n",
            "Epoch [153/400] Loss = 2.1963210105895996 TrainAccuracy = 0.21875 ValAccuracy = 0.2277\n",
            "Epoch [154/400] Loss = 2.2133357524871826 TrainAccuracy = 0.21875 ValAccuracy = 0.209\n",
            "Epoch [155/400] Loss = 2.200828790664673 TrainAccuracy = 0.25 ValAccuracy = 0.2251\n",
            "Epoch [156/400] Loss = 2.203587532043457 TrainAccuracy = 0.234375 ValAccuracy = 0.226\n",
            "Epoch [157/400] Loss = 2.1908934116363525 TrainAccuracy = 0.28125 ValAccuracy = 0.2277\n",
            "Epoch [158/400] Loss = 2.259505271911621 TrainAccuracy = 0.140625 ValAccuracy = 0.2204\n",
            "Epoch [159/400] Loss = 2.215132236480713 TrainAccuracy = 0.25 ValAccuracy = 0.2228\n",
            "Epoch [160/400] Loss = 2.252012252807617 TrainAccuracy = 0.21875 ValAccuracy = 0.2277\n",
            "160 saved\n",
            "Epoch [161/400] Loss = 2.2469301223754883 TrainAccuracy = 0.203125 ValAccuracy = 0.2184\n",
            "Epoch [162/400] Loss = 2.2432916164398193 TrainAccuracy = 0.1875 ValAccuracy = 0.2252\n",
            "Epoch [163/400] Loss = 2.1740658283233643 TrainAccuracy = 0.296875 ValAccuracy = 0.2149\n",
            "Epoch [164/400] Loss = 2.281954765319824 TrainAccuracy = 0.125 ValAccuracy = 0.2227\n",
            "Epoch [165/400] Loss = 2.2148797512054443 TrainAccuracy = 0.25 ValAccuracy = 0.2074\n",
            "Epoch [166/400] Loss = 2.2475533485412598 TrainAccuracy = 0.1875 ValAccuracy = 0.2239\n",
            "Epoch [167/400] Loss = 2.188647508621216 TrainAccuracy = 0.21875 ValAccuracy = 0.2244\n",
            "Epoch [168/400] Loss = 2.2293264865875244 TrainAccuracy = 0.203125 ValAccuracy = 0.2257\n",
            "Epoch [169/400] Loss = 2.2112133502960205 TrainAccuracy = 0.265625 ValAccuracy = 0.2224\n",
            "Epoch [170/400] Loss = 2.2143988609313965 TrainAccuracy = 0.234375 ValAccuracy = 0.2233\n",
            "170 saved\n",
            "Epoch [171/400] Loss = 2.1941559314727783 TrainAccuracy = 0.234375 ValAccuracy = 0.2161\n",
            "Epoch [172/400] Loss = 2.2774503231048584 TrainAccuracy = 0.171875 ValAccuracy = 0.2165\n",
            "Epoch [173/400] Loss = 2.2103471755981445 TrainAccuracy = 0.1875 ValAccuracy = 0.2187\n",
            "Epoch [174/400] Loss = 2.311166286468506 TrainAccuracy = 0.125 ValAccuracy = 0.215\n",
            "Epoch [175/400] Loss = 2.2230889797210693 TrainAccuracy = 0.25 ValAccuracy = 0.2235\n",
            "Epoch [176/400] Loss = 2.208418607711792 TrainAccuracy = 0.234375 ValAccuracy = 0.2237\n",
            "Epoch [177/400] Loss = 2.2330992221832275 TrainAccuracy = 0.15625 ValAccuracy = 0.2118\n",
            "Epoch [178/400] Loss = 2.1936323642730713 TrainAccuracy = 0.25 ValAccuracy = 0.2198\n",
            "Epoch [179/400] Loss = 2.163703441619873 TrainAccuracy = 0.28125 ValAccuracy = 0.224\n",
            "Epoch [180/400] Loss = 2.2296011447906494 TrainAccuracy = 0.21875 ValAccuracy = 0.2223\n",
            "180 saved\n",
            "Epoch [181/400] Loss = 2.238001823425293 TrainAccuracy = 0.203125 ValAccuracy = 0.2253\n",
            "Epoch [182/400] Loss = 2.2522575855255127 TrainAccuracy = 0.171875 ValAccuracy = 0.2168\n",
            "Epoch [183/400] Loss = 2.210512161254883 TrainAccuracy = 0.296875 ValAccuracy = 0.2256\n",
            "Epoch [184/400] Loss = 2.235867500305176 TrainAccuracy = 0.171875 ValAccuracy = 0.2229\n",
            "Epoch [185/400] Loss = 2.2779173851013184 TrainAccuracy = 0.15625 ValAccuracy = 0.2095\n",
            "Epoch [186/400] Loss = 2.2216312885284424 TrainAccuracy = 0.234375 ValAccuracy = 0.2258\n",
            "Epoch [187/400] Loss = 2.187990188598633 TrainAccuracy = 0.28125 ValAccuracy = 0.2229\n",
            "Epoch [188/400] Loss = 2.2297279834747314 TrainAccuracy = 0.25 ValAccuracy = 0.219\n",
            "Epoch [189/400] Loss = 2.255385637283325 TrainAccuracy = 0.203125 ValAccuracy = 0.2254\n",
            "Epoch [190/400] Loss = 2.27903413772583 TrainAccuracy = 0.1875 ValAccuracy = 0.2162\n",
            "190 saved\n",
            "Epoch [191/400] Loss = 2.1216650009155273 TrainAccuracy = 0.28125 ValAccuracy = 0.2275\n",
            "Epoch [192/400] Loss = 2.2296853065490723 TrainAccuracy = 0.234375 ValAccuracy = 0.2223\n",
            "Epoch [193/400] Loss = 2.224965810775757 TrainAccuracy = 0.203125 ValAccuracy = 0.2283\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA8x-XHsMUUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mapbinnet = BinResNet(BinBlock, [3, 3, 3], 10, bin_fn=\"map\").to(device)\n",
        "x = torch.load(\"/content/drive/My Drive/Artificial intelligence/bigmap.th\")\n",
        "net = ResNet(BinBlock, [3, 3, 3], 10).to(device)\n",
        "net.load_state_dict(x[\"net_state_dict\"])\n",
        "y = torch.rand(3, 3, 224, 224).to(device)\n",
        "z = mapbinnet(y)\n",
        "mapbinnet_optim = torch.optim.SGD(mapbinnet.parameters(), lr = 0.1, momentum=0.9)\n",
        "net.load_state_dict(x[\"net_state_dict\"])\n",
        "mapbinnet.load_state_dict(x[\"mapbinnet_state_dict\"])\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(mapbinnet_optim, step_size=5, gamma=0.1)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAo4-j67NQqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_losses = []"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-GOtl8kX9ZH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 778
        },
        "outputId": "d554febc-035f-42ce-b7be-b6133fc47f84"
      },
      "source": [
        "# Fine tune with the noisy supervision\n",
        "num_train_epochs = 120\n",
        "for epoch in range(num_train_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        out = mapbinnet(images)\n",
        "        net_optim.zero_grad()\n",
        "        mapbinnet_optim.zero_grad()\n",
        "        # map_optim.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        noisy_loss, mapbinnet, grad_aux_W = learn(net, mapbinnet)\n",
        "\n",
        "        ce_loss = nn.CrossEntropyLoss()(out, labels)\n",
        "        ce_loss.backward()\n",
        "        nn.utils.clip_grad_value_(mapbinnet.parameters(), 1)\n",
        "\n",
        "        for (name_p, p), (name_q, q) in zip(net.named_parameters(), mapbinnet.named_parameters()):\n",
        "            if name_p==name_q:\n",
        "                if q.grad is not None:\n",
        "                    p.grad = q.grad\n",
        "                    if all([i in name_p for i in bin]):\n",
        "                        p.grad.data += grad_aux_W[name_p]\n",
        "\n",
        "        net_optim.step()\n",
        "        mapbinnet_optim.step()\n",
        "        # map_optim.step()\n",
        "        scheduler.step()\n",
        "        train_losses.append(ce_loss.to(\"cpu\").item())\n",
        "        trainaccuracy = torch.sum(labels==torch.argmax(out, dim=-1)).to(\"cpu\").item()/len(images)\n",
        "\n",
        "    with torch.no_grad():\n",
        "            x = torch.tensor([0.0], device = device)\n",
        "            for images, labels in val_loader:\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                # print(labels.shape)\n",
        "                out = mapbinnet(images)\n",
        "                # print(torch.argmax(out, dim = -1).shape)\n",
        "                x += torch.sum(labels == torch.argmax(out, dim = -1))\n",
        "            valaccuracy = x.to(\"cpu\").item()/10000\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_train_epochs}] Loss = {train_losses[-1]} TrainAccuracy = {trainaccuracy} ValAccuracy = {valaccuracy}\")\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        torch.save({\"epoch\": epoch+1, \"net_state_dict\": net.state_dict(), \"mapbinnet_state_dict\": mapbinnet.state_dict()}, \"/content/drive/My Drive/Artificial intelligence/mapbigmodel.th\")\n",
        "        print(f\"{epoch+1} saved\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/120] Loss = 2.295302152633667 TrainAccuracy = 0.078125 ValAccuracy = 0.2262\n",
            "Epoch [2/120] Loss = 2.2043542861938477 TrainAccuracy = 0.21875 ValAccuracy = 0.2254\n",
            "Epoch [3/120] Loss = 2.1670751571655273 TrainAccuracy = 0.28125 ValAccuracy = 0.2282\n",
            "Epoch [4/120] Loss = 2.2343382835388184 TrainAccuracy = 0.21875 ValAccuracy = 0.2247\n",
            "Epoch [5/120] Loss = 2.2555980682373047 TrainAccuracy = 0.1875 ValAccuracy = 0.2231\n",
            "Epoch [6/120] Loss = 2.2341737747192383 TrainAccuracy = 0.203125 ValAccuracy = 0.2261\n",
            "Epoch [7/120] Loss = 2.1567904949188232 TrainAccuracy = 0.3125 ValAccuracy = 0.2253\n",
            "Epoch [8/120] Loss = 2.188002586364746 TrainAccuracy = 0.25 ValAccuracy = 0.2238\n",
            "Epoch [9/120] Loss = 2.2232489585876465 TrainAccuracy = 0.21875 ValAccuracy = 0.2281\n",
            "Epoch [10/120] Loss = 2.2038397789001465 TrainAccuracy = 0.265625 ValAccuracy = 0.2258\n",
            "10 saved\n",
            "Epoch [11/120] Loss = 2.1856446266174316 TrainAccuracy = 0.265625 ValAccuracy = 0.2225\n",
            "Epoch [12/120] Loss = 2.2313175201416016 TrainAccuracy = 0.234375 ValAccuracy = 0.2226\n",
            "Epoch [13/120] Loss = 2.180339813232422 TrainAccuracy = 0.25 ValAccuracy = 0.2232\n",
            "Epoch [14/120] Loss = 2.2749292850494385 TrainAccuracy = 0.171875 ValAccuracy = 0.2224\n",
            "Epoch [15/120] Loss = 2.15468430519104 TrainAccuracy = 0.265625 ValAccuracy = 0.2236\n",
            "Epoch [16/120] Loss = 2.191431760787964 TrainAccuracy = 0.234375 ValAccuracy = 0.2251\n",
            "Epoch [17/120] Loss = 2.2115354537963867 TrainAccuracy = 0.171875 ValAccuracy = 0.2222\n",
            "Epoch [18/120] Loss = 2.1838364601135254 TrainAccuracy = 0.296875 ValAccuracy = 0.2252\n",
            "Epoch [19/120] Loss = 2.277235746383667 TrainAccuracy = 0.15625 ValAccuracy = 0.2235\n",
            "Epoch [20/120] Loss = 2.1829514503479004 TrainAccuracy = 0.25 ValAccuracy = 0.2205\n",
            "20 saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-c5c26679b0e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmapbinnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mnet_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mmapbinnet_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-e7db3b82077d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres_num\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-e7db3b82077d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHardtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-5eda0a44c2ca>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbin_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbin_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    787\u001b[0m                     \"cannot assign parameters before Module.__init__() call\")\n\u001b[1;32m    788\u001b[0m             \u001b[0mremove_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_non_persistent_buffers_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mregister_parameter\u001b[0;34m(self, name, param)\u001b[0m\n\u001b[1;32m    301\u001b[0m                 \"cannot assign parameter before Module.__init__() call\")\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m             raise TypeError(\"parameter name should be a string. \"\n\u001b[1;32m    305\u001b[0m                             \"Got {}\".format(torch.typename(name)))\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}